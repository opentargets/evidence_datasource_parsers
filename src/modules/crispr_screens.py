#!/usr/bin/env python
"""Wrapper for all CRISPR parsers"""

import argparse
from functools import reduce
import logging
import sys
from pyspark.sql import SparkSession, DataFrame

from common.evidence import initialize_sparksession, write_evidence_strings
from BrainCRISPR import CRISPRBrain


def crispr_brain_wrapper(spark: SparkSession, crispr_brain_mapping: str) -> DataFrame:
    """Simple wrapper around CRISPRBrain class to generate and extract evidence."""
    crispr_brain_parser = CRISPRBrain(spark, crispr_brain_mapping)
    crispr_brain_parser.create_crispr_brain_evidence()
    return crispr_brain_parser.get_dataframe()


def main(spark: SparkSession, crispr_brain_mapping: str) -> DataFrame:
    crispr_screen_evidence_sets = [
        # Generate evidence from CRISPR Brain:
        crispr_brain_wrapper(spark, crispr_brain_mapping).persist(),
        # Further datasets can be added here:
    ]

    # Combining all sources into one single dataframe:
    combined_evidence = reduce(
        lambda df1, df2: df1.unionByName(df2, allowMissingColumns=True),
        crispr_screen_evidence_sets,
    )

    logging.info(f"Total number of CRISPR screen evidence: {combined_evidence.count()}")
    return combined_evidence


def get_parser():
    """Parsing command line argument for crispr screen ingestion."""
    parser = argparse.ArgumentParser(description=__doc__)

    parser.add_argument(
        "--crispr_brain_mapping",
        help="Curation for CRISPR Brain dataset containing disease mapping and contrast.",
        type=str,
        required=True,
    )
    parser.add_argument(
        "--output",
        help="Output gzipped json file.",
        type=str,
        required=True,
    )
    parser.add_argument(
        "--log_file",
        help="Destination of the logs generated by this script. Defaults to None",
        type=str,
        nargs="?",
        default=None,
    )
    return parser


if __name__ == "__main__":
    args = get_parser().parse_args()

    # Logger initializer. If no log_file is specified, logs are written to stderr
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s %(levelname)s %(module)s - %(funcName)s: %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )
    if args.log_file:
        logging.FileHandler(args.log_file)
    else:
        logging.StreamHandler(sys.stderr)

    spark = initialize_sparksession()

    evidence_df = main(
        spark,
        args.crispr_brain_mapping,
    )

    write_evidence_strings(evidence_df, args.output)
    logging.info(f"Evidence strings have been saved to {args.output}. Exiting.")
