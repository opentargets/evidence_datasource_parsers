#!/usr/bin/env python
"""This module adds the category of why a clinical trial has stopped early to the ChEMBL evidence."""

import argparse
import logging
import sys

import pyspark.sql.functions as f

from pyspark.sql.dataframe import DataFrame

from common.evidence import initialize_sparksession, write_evidence_strings


def main(chembl_evidence: str, predictions: str, output_file: str) -> None:
    """
    This module adds the studyStopReasonCategories to the ChEMBL evidence as a result of the categorisation of the clinical trial reason to stop.
    Args:
        chembl_evidence: Input gzipped JSON with the evidence submitted by ChEMBL.
        predictions: Input JSON containing the categories of the clinical trial reason to stop. 
        Instructions for applying the ML model here: https://github.com/ireneisdoomed/stopReasons.
        output_file: Output gzipped json file containing the ChEMBL evidence with the additional studyStopReasonCategories field.
        log_file: Destination of the logs generated by this script. Defaults to None.
    """
    logging.info(f'ChEMBL evidence JSON file: {chembl_evidence}')
    logging.info(f'Classes of reason to stop table: {predictions}')

    chembl_df = spark.read.json(chembl_evidence).repartition(200).persist()
    predictions_df = (
        spark.read.json(predictions)
        .transform(prettify_subclasses)
        .distinct()
    )

    # Join datasets
    early_stopped_evd_df = (
        # Evidence with a given reason to stop is always supported by a single NCT ID
        chembl_df.filter(f.col('studyStopReason').isNotNull())
        .select(
            "*", f.explode(f.col("urls.url")).alias("nct_id")
        ).withColumn("nct_id", f.element_at(f.split(f.col("nct_id"), "%22"), -2))
        .join(predictions_df, on='nct_id', how='left').drop('nct_id')
        .distinct()
    )

    # We expect that ~10% of evidence strings have a reason to stop assigned
    # It is asserted that this fraction is between 9 and 11% of the total count
    total_count = chembl_df.count()
    early_stopped_count = early_stopped_evd_df.count()

    if not 0.08 < early_stopped_count / total_count < 0.11:
        raise AssertionError(f'The fraction of evidence with a CT reason to stop class is not as expected ({early_stopped_count / total_count}).')

    logging.info('Evidence strings have been processed. Saving...')
    enriched_chembl_df = (
        chembl_df.filter(f.col('studyStopReason').isNull())
        .unionByName(early_stopped_evd_df, allowMissingColumns=True)
    )
    assert enriched_chembl_df.count() == chembl_df.count()
    write_evidence_strings(enriched_chembl_df, output_file)

    logging.info(f'{total_count} evidence strings have been saved to {output_file}. Exiting.')

def prettify_subclasses(predictions_df: DataFrame) -> DataFrame:
    """
    List of categories must be converted formatted with a nice name.
    """

    CATEGORIESMAPPINGS = {
        'Business_Administrative': 'Business or administrative',
        'Logistics_Resources': 'Logistics or resources',
        'Covid19': 'COVID-19',
        'Safety_Sideeffects': 'Safety or side effects',
        'Endpoint_Met': 'Met endpoint',
        'Insufficient_Enrollment': 'Insufficient enrollment',
        'Negative': 'Negative',
        'Study_Design': 'Study design',
        'Invalid_Reason': 'Invalid reason',
        'Study_Staff_Moved': 'Study staff moved',
        'Another_Study': 'Another study',
        'No_Context': 'No context',
        'Regulatory': 'Regulatory',
        'Interim_Analysis': 'Interim analysis',
        'Success': 'Success',
        'Ethical_Reason': 'Ethical reason',
        'Insufficient_Data': 'Insufficient data',
        'Uncategorised': 'Uncategorised'
    }

    sub_mapping_col = f.map_from_entries(f.array(*[f.struct(f.lit(k), f.lit(v)) for k, v in CATEGORIESMAPPINGS.items()]))

    return (
        predictions_df  
        .select('nct_id', 'subclasses', sub_mapping_col.alias('prettyStopReasonsMap'))
        # Create a MapType column to convert each element of the subclasses array
        .withColumn('studyStopReasonCategories', f.expr('transform(subclasses, x -> element_at(prettyStopReasonsMap, x))'))
        .drop('subclasses', 'prettyStopReasonsMap')
    )

def get_parser():
    """Get parser object for script ChEMBL.py."""
    parser = argparse.ArgumentParser(description=__doc__)

    parser.add_argument(
        '--chembl_evidence',
        help='Input gzipped JSON with the evidence submitted by ChEMBL',
        type=str,
        required=True,
    )
    parser.add_argument(
        '--predictions',
        help='Input TSV containing the categories of the clinical trial reason to stop. Instructions for applying the ML model here: https://github.com/ireneisdoomed/stopReasons.',
        type=str,
        required=True,
    )
    parser.add_argument(
        '--output', help='Output gzipped json file following the target safety liabilities data model.',
        type=str,
        required=True
    )
    parser.add_argument(
        '--log_file',
        help='Destination of the logs generated by this script. Defaults to None',
        type=str,
        nargs='?',
        default=None
    )

    return parser


if __name__ == '__main__':
    args = get_parser().parse_args()

    # Logger initializer. If no log_file is specified, logs are written to stderr
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s %(levelname)s %(module)s - %(funcName)s: %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S',
    )
    if args.log_file:
        logging.config.fileConfig(filename=args.log_file)
    else:
        logging.StreamHandler(sys.stderr)

    global spark
    spark = initialize_sparksession()

    main(
        chembl_evidence=args.chembl_evidence,
        predictions=args.predictions,
        output_file=args.output
    )
