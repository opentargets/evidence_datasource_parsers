import os
import argparse
import numpy as np
import pandas as pd
import scanpy as sc
from pyspark.sql import SparkSession, functions as f
from pyspark.sql.types import StructType, StructField, StringType, ArrayType, DoubleType, IntegerType

class PseudobulkExpression:
    """
    Process cellxgene formatted single-cell AnnData into pseudobulked JSON.
    This class reads an AnnData object, filters it, normalizes it,
    and then aggregates the data into pseudobulk format.
    """

    @property
    def spark(self):
        if self._spark is None:
            # Initialize Spark session lazily with google cloud connector
            self._spark = SparkSession.builder \
                .appName("PseudobulkJSON") \
                .config("spark.executor.memory", "30g") \
                .config("spark.driver.memory", "25g") \
                .config("spark.memory.offHeap.enabled",True) \
                .config("spark.memory.offHeap.size","8g") \
                .getOrCreate()
        self._spark.conf.set("hive.exec.default.partition.name", "None")
        return self._spark

    @property
    def flat_schema(self):
        # the schema for the FLAT pandas slice before we do the Spark grouping
        return StructType([
            StructField("targetId", StringType(), True),
            StructField("datasourceId", StringType(), True),
            StructField("datatypeId", StringType(), True),
            StructField("unit", StringType(), True),
            StructField("tissueBiosampleId", StringType(), True),
            StructField("tissueBiosampleFromSource", StringType(), True),
            StructField("celltypeBiosampleId", StringType(), True),
            StructField("celltypeBiosampleFromSource", StringType(), True),
            StructField("expression", DoubleType(), True),
            StructField("donorId", StringType(), True),
            StructField("cellCount", IntegerType(), True),
            StructField("sex", StringType(), True),
            StructField("age", StringType(), True),
            StructField("ethnicity", StringType(), True),
        ])
    

    def __init__(self, h5ad_path, datasource_id, datatype_id, unit, json):
        self.h5ad_path     = h5ad_path
        self.datasource_id = datasource_id
        self.datatype_id   = datatype_id
        self.unit          = unit
        self._spark = None  # Lazy initialization
        self.json = json

    def read_h5ad(self):
        print(f"Reading h5ad file: {self.h5ad_path}")
        self.adata = sc.read(self.h5ad_path)
        self.adata.var_names_make_unique()
        self.adata.strings_to_categoricals()
        if self.adata.var.index.name != 'ensg':
            raise ValueError(f"Expected var index name 'ensg', got '{self.adata.var.index.name}'")

    def filter_anndata(self, min_cells=0, min_genes=0, technology='10X'):
        if min_genes > 0:
            sc.pp.filter_cells(self.adata, min_genes=min_genes)
        if min_cells > 0:
            sc.pp.filter_genes(self.adata, min_cells=min_cells)
        print(f"Keeping only cells generated by {technology}")
        self.adata = self.adata[self.adata.obs['method'] == technology]

    def normalise_anndata(self, method='logCP10K'):
        """
        Normalise the AnnData object using the specified method.

        Parameters
        ----------
        method : str, default 'logCP10K'
            The normalization method to use. If the method matches a key in
            AnnData.layers, that layer is used as the normalized data.
            If 'logCP10K', the data is normalized to a total count of 10,000 per cell
            and log-transformed using scanpy's normalize_total and log1p functions.
        """
        print(f"Normalising data using {method}")
        if method in self.adata.layers:
            self.adata.X = self.adata.layers[method]
        elif method == 'logCP10K':
            sc.pp.normalize_total(self.adata, target_sum=1e4)
            sc.pp.log1p(self.adata)


    def pseudobulk_data(self, donor_colname='donor_id', min_cells=5,
                        tissue_agg_colname=None, celltype_agg_colname=None,
                        agg_method='mean', age_colname='age', sex_colname='sex', ethnicity_colname='ethnicity'):
        adata = self.adata

        if tissue_agg_colname and celltype_agg_colname:
            annotations = adata.obs[[tissue_agg_colname, celltype_agg_colname]].drop_duplicates()
            annotations = [(row[tissue_agg_colname], row[celltype_agg_colname]) for _, row in annotations.iterrows()]
        elif tissue_agg_colname:
            annotations = [(tissue, None) for tissue in adata.obs[tissue_agg_colname].unique()]
        elif celltype_agg_colname:
            annotations = [(None, celltype) for celltype in adata.obs[celltype_agg_colname].unique()]
        else:
            raise ValueError("Provide at least one aggregation column.")

        # define a single output directory for this whole aggregation
        agg_cols = "__".join(filter(None, [tissue_agg_colname, celltype_agg_colname]))
        file_format = "json" if self.json else "parquet"
        output_dir = f"results/{self.datasource_id}/{file_format}/{agg_cols}__{agg_method}/".replace(":", "_")
        os.makedirs(output_dir, exist_ok=True)

        for tissue_id, celltype_id in annotations:
            print(f"Aggregating tissue={tissue_id}, celltype={celltype_id}")

            subset = adata
            if tissue_id:
                subset = subset[subset.obs[tissue_agg_colname] == tissue_id]
            if celltype_id:
                subset = subset[subset.obs[celltype_agg_colname] == celltype_id]

            donor_meta = []
            agg_df = pd.DataFrame()

            # Get tissue and cell type labels
            tissue_label = subset.obs['tissue'].iat[0] if tissue_agg_colname else None
            celltype_label = subset.obs['cell_type'].iat[0] if celltype_agg_colname else None

            for donor in subset.obs[donor_colname].unique():
                donor_cells = subset[subset.obs[donor_colname] == donor]
                if donor_cells.shape[0] < min_cells:
                    continue

                donor_meta.append({
                    'donorId': donor,
                    'cellCount': donor_cells.shape[0],
                    'sex': donor_cells.obs[sex_colname].iat[0] if sex_colname in donor_cells.obs and len(donor_cells.obs[sex_colname]) > 0 else np.nan,
                    'age': donor_cells.obs[age_colname].iat[0] if age_colname in donor_cells.obs and len(donor_cells.obs[age_colname]) > 0 else np.nan,
                    'ethnicity': donor_cells.obs[ethnicity_colname].iat[0] if ethnicity_colname in donor_cells.obs and len(donor_cells.obs[ethnicity_colname]) > 0 else np.nan,
                })

                matrix = donor_cells.to_df()
                vector = matrix.mean(axis=0) if agg_method == 'mean' else matrix.sum(axis=0)
                agg_df[donor] = vector

            if agg_df.empty:
                continue

            agg_df.index.name = 'targetId'
            agg_df.reset_index(inplace=True)

            melted_df = agg_df.melt(id_vars='targetId', var_name='donorId', value_name='expression')
            meta_df = pd.DataFrame(donor_meta)
            merged_df = melted_df.merge(meta_df, on='donorId')

            merged_df['datasourceId'] = self.datasource_id
            merged_df['datatypeId'] = self.datatype_id
            merged_df['unit'] = f"{agg_method} {self.unit}"
            merged_df['tissueBiosampleId'] = tissue_id.replace(':', '_') if tissue_agg_colname else None
            merged_df['celltypeBiosampleId'] = celltype_id.replace(':', '_') if celltype_agg_colname else None
            merged_df['tissueBiosampleFromSource'] =  tissue_label
            merged_df['celltypeBiosampleFromSource'] = celltype_label

            if merged_df.empty:
                continue

            # instead of appending to all_data, write _this_ merged_df immediately:
            self._save_chunk(merged_df, output_dir)

        print(f"Finished writing JSON to {output_dir}")

    def _save_chunk(self, df: pd.DataFrame, output_dir: str):
        # Make sure the df contains all the right columns and that they are in the right order
        for col in self.flat_schema.fieldNames():
            if col not in df.columns:
                df[col] = None  # Fill missing columns with None
        # Ensure the order of columns matches the schema
        df = df[self.flat_schema.fieldNames()]
        # Lift into Spark
        raw_sdf = self.spark.createDataFrame(df, schema=self.flat_schema)

        # Append into JSON output
        if self.json:
            raw_sdf.write.mode("append") \
                .json(output_dir)
        else:
            # If not JSON, write as parquet
            raw_sdf.write.mode("append") \
                .parquet(output_dir)

        print(f"  â†’ Appended {df.shape[0]} rows into Spark and wrote JSON chunk.")


    def main(self, args, agg_methods):
        self.read_h5ad()
        self.filter_anndata(args.min_cells, args.min_genes, args.technology)
        self.normalise_anndata(args.normalisation_method)

        for m in agg_methods:
            self.pseudobulk_data(
                args.donor_colname,
                args.aggregation_min_cells,
                tissue_agg_colname=args.tissue_agg_colname,
                age_colname=args.age_colname,
                sex_colname=args.sex_colname,
                ethnicity_colname=args.ethnicity_colname,
                agg_method=m
            )
            self.pseudobulk_data(
                args.donor_colname,
                args.aggregation_min_cells,
                celltype_agg_colname=args.celltype_agg_colname,
                age_colname=args.age_colname,
                sex_colname=args.sex_colname,
                ethnicity_colname=args.ethnicity_colname,
                agg_method=m
            )
            self.pseudobulk_data(
                donor_colname=args.donor_colname,
                min_cells=args.aggregation_min_cells,
                tissue_agg_colname=args.tissue_agg_colname,
                celltype_agg_colname=args.celltype_agg_colname,
                age_colname=args.age_colname,
                sex_colname=args.sex_colname,
                ethnicity_colname=args.ethnicity_colname,
                agg_method=m
            )
        print("All done.")


if __name__ == "__main__":
    p = argparse.ArgumentParser()
    p.add_argument(
        "--h5ad_path", required=True
        )
    p.add_argument(
        "--min_cells", type=int, default=5,
        help="Minimum number of cells per gene to keep in the dataset."
        )
    p.add_argument(
        "--min_genes", type=int, default=5,
        help="Minimum number of genes per cell to keep in the dataset."
        )
    p.add_argument(
        "--technology", type=str, default='10X',
        help="Technology used for the dataset."
        )
    p.add_argument(
        "--normalisation_method", type=str, default='logCP10K',
        help="Normalisation method to use."
        )
    p.add_argument(
        "--aggregation_min_cells", type=int, default=5,
        help="Minimum number of cells per donor to keep in the pseudobulk aggregation."
        )
    p.add_argument(
        "--aggregation_method", type=str, default='mean', 
        help="Aggregation method(s) to use, e.g. 'mean' or 'mean,sum'"
        )
    p.add_argument(
        "--datasource_id", type=str, required=True,
        help="Datasource ID for the pseudobulk expression data."
        )
    p.add_argument(
        "--datatype_id", type=str, default="scrna-seq",
        help="Datatype ID for the pseudobulk expression data."
        )
    p.add_argument(
        "--json", action='store_true', default=False,
        help="Save output as JSON instead of parquet.",
    )
    p.add_argument(
        "--donor_colname", type=str, default='donor_id',
        help="Column name for the donor ID."
        )
    p.add_argument(
        "--tissue_agg_colname", type=str, default='tissue_ontology_term_id',
        help="Column name for the tissue aggregation."
        )
    p.add_argument(
        "--celltype_agg_colname", type=str, default='cell_type_ontology_term_id',
        help="Column name for the cell type aggregation."
        )
    p.add_argument(
        "--age_colname", type=str, default='age',
        help="Column name for the age."
        )
    p.add_argument(
        "--sex_colname", type=str, default='sex',
        help="Column name for the sex."
        )
    p.add_argument(
        "--ethnicity_colname", type=str, default='ethnicity',
        help="Column name for the ethnicity."
        )

    args = p.parse_args()
    methods = args.aggregation_method.split(',') 
    
    PseudobulkExpression(
        args.h5ad_path,
        datasource_id = args.datasource_id,
        datatype_id   = args.datatype_id,
        unit          = args.normalisation_method,
        json           = args.json
    ).main(args, methods)

# For example usage, uncomment the following lines:
# agg_methods = ['mean']
# pe = PseudobulkExpression(
#     '/home/alegbe/data/tabula_sapiens-small_intestine-CxG.h5ad',
#     datasource_id = 'tabula_sapiens',
#     datatype_id   = 'scrna-seq',
#     unit          = 'logCP10K',
#     biosample_index_path = 'gs://open-targets-data-releases/25.06/output/biosample'
# )
# pe.read_h5ad()
# pe.filter_anndata(min_cells=5, min_genes=0, technology='10X')
# pe.normalise_anndata(method='logCP10K')
# pe.pseudobulk_data(
#     donor_colname='donor_id',
#     min_cells=5,
#     tissue_agg_colname='tissue_ontology_term_id',
#     agg_method='mean'
# )
