import numpy as np
import pandas as pd
import scanpy as sc
import os
import argparse


class PseudobulkExpression:
    """Collection of steps to process single-cell expression into pseudobulked data."""
    
    def read_h5ad(self):
        """Read and preprocess source h5ad data."""
        print(f"Reading h5ad file: {self.h5ad_path}")
        self.adata = sc.read(self.h5ad_path)
        # Ensembl gene IDs are used as index throughout the processing code.
        self.adata.var_names_make_unique()

        self.adata.strings_to_categoricals()
        # Check AnnData object for some basic requirements.
        if self.adata.var.index.name != 'ensg':
            raise ValueError(f"Expected index name 'ensg', got '{adata.var.index.name}'")
        

    def filter_anndata(self,min_cells=0,min_genes=0,method='10X'):
        """Filter out undesired cells and genes.
        min_cells: Minimum number of cells a gene must be expressed in.
        min_genes: Minimum number of genes a cell must express.
        method: The method used to generate the data, e.g. 10X, SmartSeq2.
        """
        # print(f"Filtering cells with fewer than {min_genes} genes and genes expressed in fewer than {min_cells} cells")
        if min_genes > 0:
            sc.pp.filter_cells(self.adata, min_genes=min_genes)
        if min_cells > 0:
            sc.pp.filter_genes(self.adata, min_cells=min_cells)
        print(f"Removing cells which were not generated by {method}")
        self.adata = self.adata[self.adata.obs['method'] == method]

    def normalise_anndata(self,method='logcp10k'):
        """Normalise the AnnData object.
        method: The normalisation method e.g. logcp10k, scTransform. If it already exists in adata.layers, it will be used.
        """
        print(f"Normalising data using {method}")
        if method in self.adata.layers:
            self.adata.X = self.adata.layers[method]
        elif method == 'logcp10k':
            sc.pp.normalize_total(self.adata, target_sum=1e4)
            sc.pp.log1p(self.adata)
    
    def combine_annotations(self,annotation_colnames):
        """Combine annotations.
        annotation_colnames: List of column names to combine.
        """
        adata = self.adata
        combined_colname = '__'.join(annotation_colnames)
        # Combine annotation columns into a single column.
        adata.obs[combined_colname] = adata.obs[annotation_colnames].astype(str).agg('__'.join, axis=1)

    def make_annotation_translation_map(self,organ_column_dict,cell_type_column_dict):
        """
        Create a translation map for labels.
        organ_column_dict: Dictionary of organ columns e.g. {'tissue':'tissue_ontology_term_id'}
        cell_type_column_dict: Dictionary of cell type columns e.g. {'cell_type__tissue':'cell_type_ontology_term_id'}
        """
        # Structure of the translation map is based on gs://open-targets-pre-data-releases/2503-testrun-3/intermediate/expression/tissue-translation-map.parquet
        # +-------------------------------------+--------------+-----------------------------------------------+---------------------+-----------------------------------------------+
        # |anatomical_systems                   |efo_code      |label                                          |organs               |tissue_id                                      |
        # +-------------------------------------+--------------+-----------------------------------------------+---------------------+-----------------------------------------------+
        # |[immune system, hematopoietic system]|CL_0000939    |cytotoxic CD56-dim natural killer cell         |[immune organ, blood]|cytotoxic CD56-dim natural killer cell         |
        # |[integumental system]                |UBERON_0004264|lower leg skin                                 |[skin of body]       |lower leg skin                                 |
        # |[immune system, hematopoietic system]|CL_0002057    |CD14-positive, CD16-negative classical monocyte|[immune organ, blood]|CD14-positive, CD16-negative classical monocyte|
        # +-------------------------------------+--------------+-----------------------------------------------+---------------------+-----------------------------------------------+
        obs_df = self.adata.obs
        translation_map = pd.DataFrame([], columns=['anatomical_systems', 'efo_code', 'label', 'organs', 'tissue_id'])
        for organ_column, organ_efo_code_column in organ_column_dict.items():
            # Slide obs for unique values in the organ column and the corresponding EFO code
            organ_translation_map = obs_df[[organ_column, organ_efo_code_column]].drop_duplicates()
            # For the anatomical_systems column, I will make all rows ['immmune system'] for now
            organ_translation_map['anatomical_systems'] = [['immune system']] * organ_translation_map.shape[0]
            #Â For the organ column I will take the value from each row in the organ column but as a list e.g. 'blood' -> ['blood']
            organ_translation_map['organs'] = [[organ] for organ in organ_translation_map[organ_column]]
            organ_translation_map['tissue_id'] = organ_translation_map[organ_column]
            organ_translation_map = organ_translation_map.rename(columns={organ_column:'label', organ_efo_code_column:'efo_code'})
            translation_map = pd.concat([translation_map, organ_translation_map])
        # Slightly different approach for cell type as each cell type belongs to a tissue
        for cell_type_column, cell_type_efo_code_column in cell_type_column_dict.items():
            cell_type_translation_map = obs_df[[cell_type_column, cell_type_efo_code_column]].drop_duplicates()
            # For the anatomical_systems column, I will make all rows ['immmune system'] for now
            cell_type_translation_map['anatomical_systems'] = [['immune system']] * cell_type_translation_map.shape[0]
            # To get the organ I will split the cell type column by '__' and take the first element and
            # put it in a list e.g. 'lung__CD4 T cell' -> ['lung']
            cell_type_translation_map['organs'] = cell_type_translation_map[cell_type_column].str.split('__').apply(lambda x: [x[0]])
            cell_type_translation_map['tissue_id'] = cell_type_translation_map[cell_type_column]
            cell_type_translation_map = cell_type_translation_map.rename(columns={cell_type_column:'label', cell_type_efo_code_column:'efo_code'})
            translation_map = pd.concat([translation_map, cell_type_translation_map])
        # Save the translation map to a file
        os.makedirs('results', exist_ok=True)
        translation_map.to_parquet('~/results/translation_map.parquet', index=False)



    def pseudobulk_data(self,aggregation_colname,donor_colname,min_cells,method='dMean'):
        """Calculate pseudobulk data.
        aggregation_colname: The annotation column name to aggregate on.
        donor_colname: The donor column name to aggregate on.
        min_cells: Minimum number of cells that an annotation-donor combination must have to be included.
        method: The method used to aggregate the data, e.g. dMean, dSum.
        """
        # Code adapted from https://github.com/wtsi-hgi/QTLight/blob/main/bin/aggregate_sc_data.py
        adata = self.adata
        print(aggregation_colname)
        print("----------")

        try:
            aggregation_column = self.adata.obs[aggregation_colname]
        except KeyError:
            print(f"Aggregation column {aggregation_colname} doesn't exist in AnnData object")
            return
        # Loop through each unique annotation in the aggregation column and aggregate the data.
        annotations = adata.obs[aggregation_colname].unique()
        n_annotations = len(annotations)
        for i, annotation in enumerate(annotations):
            aggregated_data=pd.DataFrame()
        
            print(f"-----{i+1}/{n_annotations}-----")
            print(f"Aggregating data for {annotation}")

            output_dir = f'results/pseudobulk/{aggregation_colname}/{method}'
            file_path = f'{output_dir}/{annotation}.tsv'
 
            if os.path.exists(file_path):
                print(f"File {file_path} already exists, skipping")
                continue


            # Slice the AnnData object to only include the current annotation.
            annot_adata = adata[adata.obs[aggregation_colname]==annotation]
            annot_index = set(adata[adata.obs[aggregation_colname]==annotation].obs.index)
            aggregated_data_pre=pd.DataFrame()

            # Perform the aggregation for each donor.
            for donor in annot_adata.obs[donor_colname].unique():
                donor_index = set(adata[adata.obs[donor_colname]==donor].obs.index)
                annot_donor_index = set(annot_index.intersection(donor_index))
                donor_adata = adata[adata.obs.index.isin(annot_donor_index)]
                if donor_adata.obs.shape[0] >= min_cells:
                    if (method =='dSum'):
                        f = donor_adata.to_df()
                        data_aggregated_for_annot_and_individual = pd.DataFrame(f.sum(axis = 0))
                        data_aggregated_for_annot_and_individual.set_index(f.columns,inplace=True)
                    elif (method =='dMean'):
                        f = donor_adata.to_df()
                        data_aggregated_for_annot_and_individual = pd.DataFrame(f.mean(axis = 0))
                        data_aggregated_for_annot_and_individual.set_index(f.columns,inplace=True)
                    else:
                        raise ValueError('Wrong method specified, please use dMean or dSum')
                    data_aggregated_for_annot_and_individual.rename(columns={0:donor},inplace=True)
                    aggregated_data_pre=pd.concat([aggregated_data_pre,data_aggregated_for_annot_and_individual],axis=1)
            aggregated_data=pd.concat([aggregated_data,aggregated_data_pre],axis=1)

            # Save the aggregated data to a file.
            os.makedirs(output_dir, exist_ok=True)
            aggregated_data.to_csv(f'{output_dir}/{annotation}.tsv',
                                   sep='\t', index=True)

    def main(self):
        self.read_h5ad()
        # self.filter_h5ad(min_cells=3,min_genes=200,method='10X')
        self.filter_anndata(min_cells=0,min_genes=0,method='10X')
        self.normalise_anndata()
        self.combine_annotations(['tissue','cell_type'])
        self.make_annotation_translation_map({'tissue':'tissue_ontology_term_id'},{'tissue__cell_type':'cell_type_ontology_term_id'})
        for annotation in ['tissue', 'tissue__cell_type', 'cell_type']:
            for method in ['dMean','dSum']:
                self.pseudobulk_data(aggregation_colname=annotation,
                                     donor_colname='donor_id',
                                     min_cells=5,
                                     method=method)
        print("Pseudobulk expression completed")

    def __init__(self, h5ad_path):
        self.h5ad_path = h5ad_path

parser = argparse.ArgumentParser()
parser.add_argument(
    "--h5ad_path", required=True, type=str, help="Input h5ad file."
)

if __name__ == "__main__":
    args = parser.parse_args()
    PseudobulkExpression(args.h5ad_path).main()
    
