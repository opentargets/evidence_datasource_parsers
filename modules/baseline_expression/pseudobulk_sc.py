import numpy as np
import pandas as pd
import scanpy as sc
import os
import argparse
import multiprocessing
from anndata import AnnData


class PseudobulkExpression:
    """Collection of steps to process single-cell expression into pseudobulked data."""
    
    def read_h5ad(self):
        """Read and preprocess source h5ad data."""
        print(f"Reading h5ad file: {self.h5ad_path}")
        self.adata = sc.read(self.h5ad_path)
        # Ensembl gene IDs are used as index throughout the processing code.
        self.adata.var_names_make_unique()

        self.adata.strings_to_categoricals()
        # Check AnnData object for some basic requirements.
        if self.adata.var.index.name != 'ensg':
            raise ValueError(f"Expected index name 'ensg', got '{self.adata.var.index.name}'")
        

    def filter_anndata(self,min_cells=0,min_genes=0,method='10X'):
        """Filter out undesired cells and genes.
        min_cells: Minimum number of cells a gene must be expressed in.
        min_genes: Minimum number of genes a cell must express.
        method: The method used to generate the data, e.g. 10X, SmartSeq2.
        """
        # print(f"Filtering cells with fewer than {min_genes} genes and genes expressed in fewer than {min_cells} cells")
        if min_genes > 0:
            sc.pp.filter_cells(self.adata, min_genes=min_genes)
        if min_cells > 0:
            sc.pp.filter_genes(self.adata, min_cells=min_cells)
        print(f"Removing cells which were not generated by {method}")
        self.adata = self.adata[self.adata.obs['method'] == method]

    def normalise_anndata(self,method='logcp10k'):
        """Normalise the AnnData object.
        method: The normalisation method e.g. logcp10k, scTransform. If it already exists in adata.layers, it will be used.
        """
        print(f"Normalising data using {method}")
        if method in self.adata.layers:
            self.adata.X = self.adata.layers[method]
        elif method == 'logcp10k':
            sc.pp.normalize_total(self.adata, target_sum=1e4)
            sc.pp.log1p(self.adata)
    
    def combine_annotations(self,annotation_colnames):
        """Combine annotations.
        annotation_colnames: List of column names to combine.
        """
        adata = self.adata
        combined_colname = '::'.join(annotation_colnames)
        # Combine annotation columns into a single column.
        adata.obs[combined_colname] = adata.obs[annotation_colnames].astype(str).agg('::'.join, axis=1)

    def make_annotation_translation_map(self,organ_column_dict,cell_type_column_dict):
        """
        Create a translation map for labels.
        organ_column_dict: Dictionary of organ columns e.g. {'tissue':'tissue_ontology_term_id'}
        cell_type_column_dict: Dictionary of cell type columns e.g. {'cell_type::tissue':'cell_type_ontology_term_id'}
        """
        # Structure of the translation map is based on gs://open-targets-pre-data-releases/2503-testrun-3/intermediate/expression/tissue-translation-map.parquet
        # +-------------------------------------+--------------+-----------------------------------------------+---------------------+-----------------------------------------------+
        # |anatomical_systems                   |efo_code      |label                                          |organs               |tissue_id                                      |
        # +-------------------------------------+--------------+-----------------------------------------------+---------------------+-----------------------------------------------+
        # |[immune system, hematopoietic system]|CL_0000939    |cytotoxic CD56-dim natural killer cell         |[immune organ, blood]|cytotoxic CD56-dim natural killer cell         |
        # |[integumental system]                |UBERON_0004264|lower leg skin                                 |[skin of body]       |lower leg skin                                 |
        # |[immune system, hematopoietic system]|CL_0002057    |CD14-positive, CD16-negative classical monocyte|[immune organ, blood]|CD14-positive, CD16-negative classical monocyte|
        # +-------------------------------------+--------------+-----------------------------------------------+---------------------+-----------------------------------------------+
        obs_df = self.adata.obs
        translation_map = pd.DataFrame([], columns=['anatomical_systems', 'efo_code', 'label', 'organs', 'tissue_id'])
        for organ_column, organ_efo_code_column in organ_column_dict.items():
            # Slide obs for unique values in the organ column and the corresponding EFO code
            organ_translation_map = obs_df[[organ_column, organ_efo_code_column]].drop_duplicates()
            # For the anatomical_systems column, I will make all rows ['immmune system'] for now
            organ_translation_map['anatomical_systems'] = ['immune system']
            organ_translation_map['efo_code'] = organ_translation_map[organ_efo_code_column]
            organ_translation_map['label'] = organ_translation_map[organ_column]
            #Â For the organ column I will take the value from each row in the organ column but as a list e.g. 'blood' -> ['blood']
            organ_translation_map['organs'] = [[organ] for organ in organ_translation_map[organ_column]]
            organ_translation_map['tissue_id'] = organ_translation_map[organ_column]
            translation_map = pd.concat([translation_map, organ_translation_map])
        # Slightly different approach for cell type as each cell type belongs to a tissue
        for cell_type_column, cell_type_efo_code_column in cell_type_column_dict.items():
            cell_type_translation_map = obs_df[[cell_type_column, cell_type_efo_code_column]].drop_duplicates()
            # For the anatomical_systems column, I will make all rows ['immmune system'] for now
            cell_type_translation_map['anatomical_systems'] = ['immune system']
            cell_type_translation_map['efo_code'] = cell_type_translation_map[cell_type_efo_code_column]
            cell_type_translation_map['label'] = cell_type_translation_map[cell_type_column]
            # To get the organ I will split the cell type column by '::' and take the first element and
            # put it in a list e.g. 'lung::CD4 T cell' -> ['lung']
            cell_type_translation_map['organs'] = cell_type_translation_map[cell_type_column].str.split('::').apply(lambda x: [x[0]])
            cell_type_translation_map['tissue_id'] = cell_type_translation_map[cell_type_column]
            translation_map = pd.concat([translation_map, cell_type_translation_map])
        # Save the translation map to a file
        os.makedirs('results', exist_ok=True)
        translation_map.to_parquet('results/translation_map.parquet')

    def create_minimal_anndata(self, obs, var, layers=None, uns=None, obsm=None, varm=None, obsp=None, raw=False):
        """Create a minimal AnnData object with only the specified attributes."""
        new_adata = AnnData(X=self.adata.X, obs=self.adata.obs[obs], var=self.adata.var[var])
        # Optionally, copy additional attributes if provided.
        if layers:
            new_adata.layers = {k: self.adata.layers[k] for k in layers if k in self.adata.layers}
        if uns:
            new_adata.uns = {k: self.adata.uns[k] for k in uns if k in self.adata.uns}
        if obsm:
            new_adata.obsm = {k: self.adata.obsm[k] for k in obsm if k in self.adata.obsm}
        if varm:
            new_adata.varm = {k: self.adata.varm[k] for k in varm if k in self.adata.varm}
        if obsp:
            new_adata.obsp = {k: self.adata.obsp[k] for k in obsp if k in self.adata.obsp}
        if raw and self.adata.raw is not None:
            new_adata.raw = self.adata.raw
        
        # Ensure categoricals are set.
        new_adata.strings_to_categoricals()

        del self.adata  # Free up memory

        self.adata = new_adata
        
    def process_annotation(self, annotation):
        """
        Worker function to process one annotation for pseudobulking.

        annotation: The annotation to process.
        """
        adata = self.adata  # Use the AnnData from the instance
        output_dir = f'results/pseudobulk/{self.aggregation_colname}/{self.method}'
        file_path = f'{output_dir}/{annotation}.tsv'

        if os.path.exists(file_path):
            print(f"File {file_path} already exists, skipping")
            return

        print(f"Aggregating data for {annotation}")

        # Subset by the current annotation.
        annot_adata = adata[adata.obs[self.aggregation_colname] == annotation]
        aggregated_data = pd.DataFrame()

        # Loop over donors within the annotation.
        for donor in annot_adata.obs[self.donor_colname].unique():
            # Subset further by donor.
            donor_adata = annot_adata[annot_adata.obs[self.donor_colname] == donor]
            if donor_adata.obs.shape[0] >= self.min_cells:
                f = donor_adata.to_df()
                if self.method == 'dSum':
                    data_aggregated = pd.DataFrame(f.sum(axis=0))
                elif self.method == 'dMean':
                    data_aggregated = pd.DataFrame(f.mean(axis=0))
                else:
                    raise ValueError('Wrong method specified, please use dMean or dSum')
                # Rename the aggregated column to the donor's name.
                data_aggregated.rename(columns={0: donor}, inplace=True)
                aggregated_data = pd.concat([aggregated_data, data_aggregated], axis=1)

        if aggregated_data.shape[0] == 0:
            print(f"No pseudobulked data for {annotation}, with {annot_adata.obs.shape[0]} cells. Skipping.")
            return

        os.makedirs(output_dir, exist_ok=True)
        aggregated_data.to_csv(file_path, sep='\t', index=True)

    def pseudobulk_data(self, aggregation_colname, donor_colname, min_cells, method='dMean'):
        """Calculate pseudobulk data in parallel.
        aggregation_colname: The annotation column name to aggregate on.
        donor_colname: The donor column name to aggregate on.
        min_cells: Minimum number of cells that an annotation-donor combination must have to be included.
        method: The method used to aggregate the data, e.g. dMean, dSum.
        """

        self.aggregation_colname = aggregation_colname
        self.donor_colname = donor_colname
        self.min_cells = min_cells
        self.method = method
        
        # List all unique annotations.
        print(aggregation_colname)
        print("----------")

        try:
            aggregation_column = self.adata.obs[aggregation_colname]
        except KeyError:
            print(f"Aggregation column {aggregation_colname} doesn't exist in AnnData object")
            return
        # Loop through each unique annotation in the aggregation column and aggregate the data.
        annotations = adata.obs[aggregation_colname].unique()
        n_annotations = len(annotations)
        for i, annotation in enumerate(annotations):
            aggregated_data=pd.DataFrame()
        
            print(f"-----{i+1}/{n_annotations}-----")
            print(f"Aggregating data for {annotation}")

            output_dir = f'results/pseudobulk/{aggregation_colname}/{method}'
            file_path = f'{output_dir}/{annotation}.tsv'
 
            if os.path.exists(file_path):
                print(f"File {file_path} already exists, skipping")
                continue

            

        # Launch a multiprocessing pool and process annotations in parallel.
        pool = multiprocessing.Pool(processes=4)
        pool.map(self.process_annotation, annotations)
        pool.close()
        pool.join()

            # Perform the aggregation for each donor.
            for donor in annot_adata.obs[donor_colname].unique():
                donor_index = set(adata[adata.obs[donor_colname]==donor].obs.index)
                annot_donor_index = set(annot_index.intersection(donor_index))
                donor_adata = adata[adata.obs.index.isin(annot_donor_index)]
                if donor_adata.obs.shape[0] >= min_cells:
                    if (method =='dSum'):
                        f = donor_adata.to_df()
                        data_aggregated_for_annot_and_individual = pd.DataFrame(f.sum(axis = 0))
                        data_aggregated_for_annot_and_individual.set_index(f.columns,inplace=True)
                    elif (method =='dMean'):
                        f = donor_adata.to_df()
                        data_aggregated_for_annot_and_individual = pd.DataFrame(f.mean(axis = 0))
                        data_aggregated_for_annot_and_individual.set_index(f.columns,inplace=True)
                    else:
                        raise ValueError('Wrong method specified, please use dMean or dSum')
                    data_aggregated_for_annot_and_individual.rename(columns={0:donor},inplace=True)
                    aggregated_data_pre=pd.concat([aggregated_data_pre,data_aggregated_for_annot_and_individual],axis=1)
            aggregated_data=pd.concat([aggregated_data,aggregated_data_pre],axis=1)

            # Save the aggregated data to a file.
            os.makedirs(output_dir, exist_ok=True)
            aggregated_data.to_csv(f'{output_dir}/{annotation}.tsv',
                                   sep='\t', index=True)

    def main(self):
        self.read_h5ad()
        # self.filter_h5ad(min_cells=3,min_genes=200,method='10X')
        self.filter_anndata(min_cells=0,min_genes=0,method='10X')
        self.normalise_anndata()
        self.combine_annotations(['tissue','cell_type'])
        for annotation in ['tissue', 'tissue::cell_type', 'cell_type']:
            for method in ['dMean','dSum']:
                self.pseudobulk_data(aggregation_colname=annotation,
                                     donor_colname='donor_id',
                                     min_cells=5,
                                     method=method)
        print("Pseudobulk expression completed")

    def __init__(self, h5ad_path):
        self.h5ad_path = h5ad_path

parser = argparse.ArgumentParser()
parser.add_argument(
    "--h5ad_path", required=True, type=str, help="Input h5ad file."
)

if __name__ == "__main__":
    args = parser.parse_args()
    PseudobulkExpression(args.h5ad_path).main()
    

