

import os
import argparse
import numpy as np
import pandas as pd
import scanpy as sc
from pyspark.sql import SparkSession, functions as f

class PseudobulkExpression:
    @property
    def spark(self):
        if self._spark is None:
            # Initialize Spark session lazily with google cloud connector
            self._spark = SparkSession.builder \
                .appName("PseudobulkJSON") \
                .config("spark.jars", "https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop3-latest.jar") \
                .config("spark.executor.memory", "70g") \
                .config("spark.driver.memory", "50g") \
                .config("spark.memory.offHeap.enabled",True) \
                .config("spark.memory.offHeap.size","16g") \
                .getOrCreate()
        return self._spark
    """
    Process cellxgene formatted single-cell AnnData into pseudobulked
    JSON per gene, matching the unaggregated schema.
    This class reads an AnnData object, filters it, normalizes it,
    and then aggregates the data into pseudobulk format.
    It also generates a translation map for tissue and cell type annotations.
    """

    def __init__(self, h5ad_path, datasource_id, datatype_id, unit, biosample_index_path=None):
        self.h5ad_path     = h5ad_path
        self.datasource_id = datasource_id
        self.datatype_id   = datatype_id
        self.unit          = unit
        self._spark = None  # Lazy initialization
        self.biosample_index_path = biosample_index_path
        self.biosample_index = None
        if biosample_index_path is not None:
            self.biosample_index = self.spark.read.parquet(biosample_index_path) \
                .select('biosampleId', 'biosampleName') \
                .withColumnRenamed('biosampleId', 'ontology_term_id') \
                .withColumnRenamed('biosampleName', 'label') \
                .toPandas()

    def read_h5ad(self):
        print(f"Reading h5ad file: {self.h5ad_path}")
        self.adata = sc.read(self.h5ad_path)
        self.adata.var_names_make_unique()
        self.adata.strings_to_categoricals()
        if self.adata.var.index.name != 'ensg':
            raise ValueError(f"Expected var index name 'ensg', got '{self.adata.var.index.name}'")

    def filter_anndata(self, min_cells=0, min_genes=0, technology='10X'):
        if min_genes > 0:
            sc.pp.filter_cells(self.adata, min_genes=min_genes)
        if min_cells > 0:
            sc.pp.filter_genes(self.adata, min_cells=min_cells)
        print(f"Keeping only cells generated by {technology}")
        self.adata = self.adata[self.adata.obs['method'] == technology]

    def normalise_anndata(self, method='logCP10K'):
        """
        Normalise the AnnData object using the specified method.

        Parameters
        ----------
        method : str, default 'logCP10K'
            The normalization method to use. If the method matches a key in
            AnnData.layers, that layer is used as the normalized data.
            If 'logCP10K', the data is normalized to a total count of 10,000 per cell
            and log-transformed using scanpy's normalize_total and log1p functions.
        """
        print(f"Normalising data using {method}")
        if method in self.adata.layers:
            self.adata.X = self.adata.layers[method]
        elif method == 'logCP10K':
            sc.pp.normalize_total(self.adata, target_sum=1e4)
            sc.pp.log1p(self.adata)


    def pseudobulk_data(self, donor_colname='donor_id', min_cells=5, 
                        tissue_agg_colname=None, celltype_agg_colname=None,  agg_method='dMean'):
        adata = self.adata
        if tissue_agg_colname is None and celltype_agg_colname is None:
            raise ValueError("At least one aggregation column must be specified: tissue_agg_colname or celltype_agg_colname")
        elif tissue_agg_colname is not None and celltype_agg_colname is not None:
            # Make a list of tuples for the annotations list that will be looped through e.g.
            # [('liver', 'neuron'), ('liver', 'glia'), ('ileum', 'enterocyte')]
            annotations = adata.obs[[tissue_agg_colname, celltype_agg_colname]].drop_duplicates()
            # Make a list of tuples for each unique combination
            annotations = list(zip(annotations[tissue_agg_colname], annotations[celltype_agg_colname]))
        elif tissue_agg_colname is not None:
            # If only tissue aggregation is specified, use unique tissue annotations
            annotations = adata.obs[tissue_agg_colname].unique()
            # Make into a list of tuples where the second element is None
            annotations = [(annot, None) for annot in annotations]
        elif celltype_agg_colname is not None:
            # If only cell type aggregation is specified, use unique cell type annotations
            annotations = adata.obs[celltype_agg_colname].unique()
            # Make into a list of tuples where the first element is None
            annotations = [(None, annot) for annot in annotations]

        # Ensure the donor column exists
        if donor_colname not in adata.obs.columns:
            raise ValueError(f"Donor column '{donor_colname}' not found in AnnData.obs")
        for annot in annotations:
            print(f"Aggregating {annot}")
            subset = adata
            if tissue_agg_colname:
                subset = subset[subset.obs[tissue_agg_colname] == annot[0]]
            if celltype_agg_colname:
                subset = subset[subset.obs[celltype_agg_colname] == annot[1]]
            agg_df = pd.DataFrame()
            donor_meta = []
            # per-donor aggregation
            for donor in subset.obs[donor_colname].unique():
                mask = subset.obs[donor_colname] == donor
                donor_cells = subset[mask]
                count = donor_cells.shape[0]
                if count < min_cells:
                    continue
                donor_meta.append({
                    'sampleId': donor,
                    'cellCount': count,
                    'sex':   np.nan if 'sex' not in donor_cells.obs else donor_cells.obs['sex'].iat[0],
                    'age':  np.nan if 'age' not in donor_cells.obs else donor_cells.obs['age'].iat[0],
                    'ethnicity' : np.nan if 'self_reported_ethnicity' not in donor_cells.obs else donor_cells.obs['self_reported_ethnicity'].iat[0]
                })
                matrix = donor_cells.to_df()
                vector = matrix.mean(axis=0) if agg_method=='dMean' else matrix.sum(axis=0)
                agg_df = pd.concat([agg_df, pd.DataFrame(vector, columns=[donor])], axis=1)
            # JSON all genes
            self.save_json(agg_df, donor_meta, annot, agg_method,
                           tissue_agg_colname, celltype_agg_colname)

    def save_json(self,
            agg_df,
            donor_meta,
            annotation,
            agg_method,
            tissue_agg_colname,
            celltype_agg_colname,
            per_gene=False):
            # if per_gene=True, writes one file per gene under
            # results/json/{annotation_label}/{agg_method}/{gene}.json
            # 1) wide → Spark DF
            index_name = agg_df.index.name if agg_df.index.name is not None else 'index'
            pdf = agg_df.reset_index().rename(columns={index_name: 'targetId'})
            if pdf.empty:
                print(f"No data for annotation {annotation} with method {agg_method}")
                return
            sdf = self.spark.createDataFrame(pdf)

            # 2) pivot to long: (targetId, sampleId, expression)
            sample_cols = [c for c in pdf.columns if c != 'targetId']
            N = len(sample_cols)
            packed = ", ".join([f"'{c}', `{c}`" for c in sample_cols])
            long_df = sdf.selectExpr(
                "targetId",
                f"stack({N}, {packed}) as (sampleId, expression)"
            )

            # 3) join donor metadata
            meta_df = self.spark.createDataFrame(donor_meta)
            joined = long_df.join(meta_df, on='sampleId', how='left')

            # 4) figure out tissue / celltype IDs and labels
            tissue_id, celltype_id = None, None
            tissue_label, ct_label = None, None

            if tissue_agg_colname:
                tissue_id = annotation[0]
                tissue_label = tissue_id
                # If tissue ID starts with EFO, CLO or UBERON get the label from the biosample index
                if tissue_id.startswith(('EFO', 'CLO', 'UBERON')) and self.biosample_index is not None:
                    match = self.biosample_index.loc[self.biosample_index['ontology_term_id'] == tissue_id.replace(':', '_'), 'label']
                    if not match.empty:
                        tissue_label = match.values[0]
                    
            if celltype_agg_colname:
                celltype_id = annotation[1]
                ct_label = celltype_id
                # If cell type ID starts with CL, UBERON or EFO get the label from the biosample index
                if celltype_id.startswith(('CL', 'UBERON', 'EFO')) and self.biosample_index is not None:
                    match = self.biosample_index.loc[self.biosample_index['ontology_term_id'] == celltype_id.replace(':', '_'), 'label']
                    if not match.empty:
                        ct_label = match.values[0]

            # 5) add constant columns
            consts = {
                'datasourceId':               self.datasource_id,
                'datatypeId':                 self.datatype_id,
                'unit':                       self.unit,
                'tissueBiosampleId':          tissue_id,
                'celltypeBiosampleId':        celltype_id,
                'tissueBiosampleFromSource':  tissue_label,
                'celltypeBiosampleFromSource':ct_label,
                'targetFromSource':           None
            }
            for k, v in consts.items():
                joined = joined.withColumn(k,  f.lit(v))

            # 6) build the nested struct and aggregate
            expr_struct =  f.struct(
                f.col('expression').alias('expression'),
                f.col('sampleId').alias('sampleId'),
                f.col('cellCount').alias('cellCount'),
                f.col('sex').alias('sex'),
                f.col('ethnicity').alias('ethnicity'),
                f.col('age').alias('age'),
            )
            nested = joined.withColumn('expression', expr_struct)

            group_cols = ['targetId'] + list(consts.keys())
            result = nested.groupBy(*group_cols) \
                        .agg( f.collect_list('expression').alias('expression'))

            # 7) build a friendly annotation label for the path
            parts = []
            if tissue_agg_colname:
                parts.append(tissue_id)
            if celltype_agg_colname:
                parts.append(celltype_id)
            annotation_label = "__".join(parts) or "all"
            annotation_label = annotation_label.replace(":", "_").replace("/", "_")

            base_dir = f'results/json/{agg_method}'
            os.makedirs(base_dir, exist_ok=True)
            if per_gene:
                # 1 file-per-gene via DataFrameWriter
                # -----------------------------------
                # collect the distinct gene IDs (small metadata collect only)
                gene_ids = [r.targetId for r in result.select('targetId').collect()]
                
                for gene in gene_ids:
                    safe = gene.replace('/', '_')
                    # Generate a directory for each gene and save by annotation_label
                    gene_dir = os.path.join(base_dir, safe)
                    os.makedirs(gene_dir, exist_ok=True)
                    out_path = os.path.join(gene_dir, f'{annotation_label}.json')
                    # filter down to this one gene
                    result.filter(f.col('targetId') == gene).coalesce(1).write \
                        .mode("overwrite") \
                        .json(out_path)                # this writes a _SUCCESS + part-*.json folder

                print(f"Wrote {len(gene_ids)} per‑gene JSON files under {base_dir}/")
            else:
                # single JSON‑lines file with one record per gene
                single_dir = os.path.join(base_dir, annotation_label)
                result.coalesce(1).write \
                    .mode("overwrite") \
                    .json(single_dir)

                # if you need exactly one .jsonl file instead of a folder+part-*, you can
                # move/rename the part file afterward in Python.
                cnt = result.count()
                print(f"Wrote JSONL with {cnt} gene records to {single_dir}/")



    def main(self, args, agg_methods):
        self.read_h5ad()
        self.filter_anndata(args.min_cells, args.min_genes, args.technology)
        self.normalise_anndata(args.normalisation_method)
        for m in agg_methods:
            # self.pseudobulk_data(
            #     args.donor_colname,
            #     args.aggregation_min_cells,
            #     tissue_agg_colname=args.tissue_agg_colname,
            #     agg_method=m
            # )
            # self.pseudobulk_data(
            #     args.donor_colname,
            #     args.aggregation_min_cells,
            #     celltype_agg_colname=args.celltype_agg_colname,
            #     agg_method=m
            # )
            self.pseudobulk_data(
                args.donor_colname,
                args.aggregation_min_cells,
                tissue_agg_colname=args.tissue_agg_colname,
                celltype_agg_colname=args.celltype_agg_colname,
                agg_method=m
            )
        print("Done: pseudobulk + per-gene JSON export.")


if __name__ == "__main__":
    p = argparse.ArgumentParser()
    p.add_argument("--h5ad_path", required=True)
    p.add_argument("--min_cells", type=int, default=5)
    p.add_argument("--min_genes", type=int, default=5)
    p.add_argument("--technology", type=str, default='10X')
    p.add_argument("--normalisation_method", type=str, default='logCP10K')
    p.add_argument("--donor_colname", type=str, default='donor_id')
    p.add_argument("--aggregation_min_cells", type=int, default=5)
    p.add_argument("--aggregation_method", type=str, default='dMean')
    p.add_argument("--datasource_id", type=str, required=True)
    p.add_argument("--datatype_id", type=str, default="scrna-seq")
    p.add_argument("--unit", type=str, default="logCP10K")
    p.add_argument("--biosample_index_path", type=str, required=True)
    p.add_argument("--tissue_agg_colname", type=str, default='tissue_ontology_term_id')
    p.add_argument("--celltype_agg_colname", type=str, default='cell_type_ontology_term_id')

    args = p.parse_args()
    methods = args.aggregation_method.split(',') 
    
    PseudobulkExpression(
        args.h5ad_path,
        datasource_id = args.datasource_id,
        datatype_id   = args.datatype_id,
        unit          = args.unit,
        biosample_index_path = args.biosample_index_path
    ).main(args, methods)

# For example usage, uncomment the following lines:
# agg_methods = ['dMean']
# pe = PseudobulkExpression(
#     '/home/alegbe/data/tabula_sapiens-small_intestine-CxG.h5ad',
#     datasource_id = 'tabula_sapiens',
#     datatype_id   = 'scrna-seq',
#     unit          = 'logCP10K',
#     biosample_index_path = 'gs://open-targets-data-releases/25.06/output/biosample'
# )
# pe.read_h5ad()
# pe.filter_anndata(min_cells=5, min_genes=0, technology='10X')
# pe.normalise_anndata(method='logCP10K')
# pe.pseudobulk_data(
#     donor_colname='donor_id',
#     min_cells=5,
#     tissue_agg_colname='tissue_ontology_term_id',
#     agg_method='dMean'
# )
