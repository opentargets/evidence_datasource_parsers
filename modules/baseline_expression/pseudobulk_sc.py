import os
import argparse
import numpy as np
import pandas as pd
import scanpy as sc
from pyspark.sql import SparkSession, functions as f
from pyspark.sql.types import StructType, StructField, StringType, ArrayType, DoubleType, IntegerType

class PseudobulkExpression:
    """
    Process cellxgene formatted single-cell AnnData into pseudobulked JSON.
    This class reads an AnnData object, filters it, normalizes it,
    and then aggregates the data into pseudobulk format.
    """

    @property
    def spark(self):
        if self._spark is None:
            # Initialize Spark session lazily with google cloud connector
            self._spark = SparkSession.builder \
                .appName("PseudobulkJSON") \
                .config("spark.jars", "https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop3-latest.jar") \
                .config("spark.executor.memory", "70g") \
                .config("spark.driver.memory", "50g") \
                .config("spark.memory.offHeap.enabled",True) \
                .config("spark.memory.offHeap.size","16g") \
                .getOrCreate()
        return self._spark

    @property
    def schema(self):
        return StructType([
            StructField("targetId", StringType(), True),
            StructField("datasourceId", StringType(), True),
            StructField("datatypeId", StringType(), True),
            StructField("unit", StringType(), True),
            StructField("tissueBiosampleId", StringType(), True),
            StructField("celltypeBiosampleId", StringType(), True),
            StructField("tissueBiosampleFromSource", StringType(), True),
            StructField("celltypeBiosampleFromSource", StringType(), True),
            StructField("targetFromSource", StringType(), True),
            StructField("UnaggregatedExpression", ArrayType(StructType([
                StructField("expression", DoubleType(), True),
                StructField("sampleId", StringType(), True),
                StructField("cellCount", IntegerType(), True),
                StructField("sex", StringType(), True),
                StructField("age", StringType(), True),
                StructField("ethnicity", StringType(), True),
            ])), True)
        ])

    def __init__(self, h5ad_path, datasource_id, datatype_id, unit, biosample_index_path=None):
        self.h5ad_path     = h5ad_path
        self.datasource_id = datasource_id
        self.datatype_id   = datatype_id
        self.unit          = unit
        self._spark = None  # Lazy initialization
        self.biosample_index_path = biosample_index_path
        self.biosample_index = None
        if biosample_index_path is not None:
            self.biosample_index = self.spark.read.parquet(biosample_index_path) \
                .select('biosampleId', 'biosampleName') \
                .withColumnRenamed('biosampleId', 'ontology_term_id') \
                .withColumnRenamed('biosampleName', 'label') \
                .toPandas()

    def read_h5ad(self):
        print(f"Reading h5ad file: {self.h5ad_path}")
        self.adata = sc.read(self.h5ad_path)
        self.adata.var_names_make_unique()
        self.adata.strings_to_categoricals()
        if self.adata.var.index.name != 'ensg':
            raise ValueError(f"Expected var index name 'ensg', got '{self.adata.var.index.name}'")

    def filter_anndata(self, min_cells=0, min_genes=0, technology='10X'):
        if min_genes > 0:
            sc.pp.filter_cells(self.adata, min_genes=min_genes)
        if min_cells > 0:
            sc.pp.filter_genes(self.adata, min_cells=min_cells)
        print(f"Keeping only cells generated by {technology}")
        self.adata = self.adata[self.adata.obs['method'] == technology]

    def normalise_anndata(self, method='logCP10K'):
        """
        Normalise the AnnData object using the specified method.

        Parameters
        ----------
        method : str, default 'logCP10K'
            The normalization method to use. If the method matches a key in
            AnnData.layers, that layer is used as the normalized data.
            If 'logCP10K', the data is normalized to a total count of 10,000 per cell
            and log-transformed using scanpy's normalize_total and log1p functions.
        """
        print(f"Normalising data using {method}")
        if method in self.adata.layers:
            self.adata.X = self.adata.layers[method]
        elif method == 'logCP10K':
            sc.pp.normalize_total(self.adata, target_sum=1e4)
            sc.pp.log1p(self.adata)


    def pseudobulk_data(self, donor_colname='donor_id', min_cells=5,
                        tissue_agg_colname=None, celltype_agg_colname=None,
                        agg_method='mean', age_colname='age', sex_colname='sex', ethnicity_colname='ethnicity'):
        adata = self.adata

        if tissue_agg_colname and celltype_agg_colname:
            annotations = adata.obs[[tissue_agg_colname, celltype_agg_colname]].drop_duplicates()
            annotations = [(row[tissue_agg_colname], row[celltype_agg_colname]) for _, row in annotations.iterrows()]
        elif tissue_agg_colname:
            annotations = [(tissue, None) for tissue in adata.obs[tissue_agg_colname].unique()]
        elif celltype_agg_colname:
            annotations = [(None, celltype) for celltype in adata.obs[celltype_agg_colname].unique()]
        else:
            raise ValueError("Provide at least one aggregation column.")

        all_data = []

        for tissue_id, celltype_id in annotations:
            print(f"Aggregating tissue={tissue_id}, celltype={celltype_id}")

            subset = adata
            if tissue_id:
                subset = subset[subset.obs[tissue_agg_colname] == tissue_id]
            if celltype_id:
                subset = subset[subset.obs[celltype_agg_colname] == celltype_id]

            donor_meta = []
            agg_df = pd.DataFrame()

            for donor in subset.obs[donor_colname].unique():
                donor_cells = subset[subset.obs[donor_colname] == donor]
                if donor_cells.shape[0] < min_cells:
                    continue

                donor_meta.append({
                    'sampleId': donor,
                    'cellCount': donor_cells.shape[0],
                    'sex': donor_cells.obs[sex_colname].iat[0] if sex_colname in donor_cells.obs and len(donor_cells.obs[sex_colname]) > 0 else np.nan,
                    'age': donor_cells.obs[age_colname].iat[0] if age_colname in donor_cells.obs and len(donor_cells.obs[age_colname]) > 0 else np.nan,
                    'ethnicity': donor_cells.obs[ethnicity_colname].iat[0] if ethnicity_colname in donor_cells.obs and len(donor_cells.obs[ethnicity_colname]) > 0 else np.nan
                })

                matrix = donor_cells.to_df()
                vector = matrix.mean(axis=0) if agg_method == 'mean' else matrix.sum(axis=0)
                agg_df[donor] = vector

            if agg_df.empty:
                continue

            agg_df.index.name = 'targetId'
            agg_df.reset_index(inplace=True)

            melted_df = agg_df.melt(id_vars='targetId', var_name='sampleId', value_name='expression')
            meta_df = pd.DataFrame(donor_meta)
            merged_df = melted_df.merge(meta_df, on='sampleId')

            merged_df['tissueBiosampleId'] = tissue_id
            merged_df['celltypeBiosampleId'] = celltype_id
            merged_df['datasourceId'] = self.datasource_id
            merged_df['datatypeId'] = self.datatype_id
            merged_df['unit'] = f"{agg_method} {self.unit}"

            if tissue_agg_colname:
                tissue_label = self.map_biosample_label(tissue_id) if tissue_id else None
            if celltype_agg_colname:
                ct_label = self.map_biosample_label(celltype_id) if celltype_id else None

            merged_df['tissueBiosampleFromSource'] = tissue_label if tissue_agg_colname else None
            merged_df['celltypeBiosampleFromSource'] = ct_label if celltype_agg_colname else None

            all_data.append(merged_df)

        if not all_data:
            print("No data to save.")
            return

        final_df = pd.concat(all_data, ignore_index=True)
        self.save_json(final_df, agg_method, tissue_agg_colname, celltype_agg_colname)

    def save_json(self, final_df, agg_method, tissue_agg_colname=None, celltype_agg_colname=None):
        # Output directory convention: aggregation columns are joined by double underscores (__).
        # Example: results/json/datasourceId__tissue_agg_colname__celltype_agg_colname__agg_method
        agg_cols = "__".join(filter(None, [tissue_agg_colname, celltype_agg_colname]))
        output_dir = f"results/json/{self.datasource_id}__{agg_cols}__{agg_method}"
        os.makedirs(output_dir, exist_ok=True)
        print(final_df.head())
        final_df_formatted = self.format_final_df(final_df, tissue_agg_colname, celltype_agg_colname)
        print(final_df_formatted.head())

        # Check which columns are missing compared to the schema and add them with None values
        for field in self.schema.fields:
            if field.name not in final_df_formatted.columns:
                final_df_formatted[field.name] = None
        # Ensure the order of columns matches the schema and warn if extra columns exist
        schema_cols = self.schema.fieldNames()
        extra_cols = set(final_df_formatted.columns) - set(schema_cols)
        if extra_cols:
            print(f"Warning: Extra columns not in schema will be dropped: {extra_cols}")
        final_df_formatted = final_df_formatted[schema_cols]
        print(final_df_formatted.head())

        sdf = self.spark.createDataFrame(final_df_formatted, schema=self.schema)
        sdf.write.mode('overwrite').json(output_dir)
        print(f"JSON saved to {output_dir}")

    def format_final_df(self, df, tissue_agg_colname=None, celltype_agg_colname=None):
        groupby_cols = ['targetId', 'datasourceId', 'datatypeId', 'unit']
        if celltype_agg_colname:
            groupby_cols.append('celltypeBiosampleId')
            groupby_cols.append('celltypeBiosampleFromSource')
        if tissue_agg_colname:
            groupby_cols.append('tissueBiosampleId')
            groupby_cols.append('tissueBiosampleFromSource')
        grouped_df = (
            df
            .apply(lambda x: x[
                [col for col in ['expression', 'sampleId', 'cellCount', 'sex', 'age', 'ethnicity'] if col in x.columns]
            ].to_dict('records'))
            ]].to_dict('records'))
            .reset_index(drop=False)
            .rename(columns={0: 'UnaggregatedExpression'})
        )

        return grouped_df

    def map_biosample_label(self, biosample_label):
        # If cell type ID starts with CL, UBERON or EFO get the label from the biosample index
        if biosample_label.startswith(('CL', 'UBERON', 'EFO')) and self.biosample_index is not None:
            biosample_label = biosample_label.replace(':', '_')
            match = self.biosample_index.loc[self.biosample_index['ontology_term_id'] == biosample_label, 'label']
            if not match.empty:
                return match.values[0]
        # Otherwise return the biosample label as is
        return biosample_label



    def main(self, args, agg_methods):
        self.read_h5ad()
        self.filter_anndata(args.min_cells, args.min_genes, args.technology)
        self.normalise_anndata(args.normalisation_method)

        for m in agg_methods:
            self.pseudobulk_data(
                args.donor_colname,
                args.aggregation_min_cells,
                tissue_agg_colname=args.tissue_agg_colname,
                agg_method=m
            )
            self.pseudobulk_data(
                args.donor_colname,
                args.aggregation_min_cells,
                celltype_agg_colname=args.celltype_agg_colname,
                agg_method=m
            )
            self.pseudobulk_data(
                donor_colname=args.donor_colname,
                min_cells=args.aggregation_min_cells,
                tissue_agg_colname=args.tissue_agg_colname,
                celltype_agg_colname=args.celltype_agg_colname,
                agg_method=m
            )
        print("All done.")


if __name__ == "__main__":
    p = argparse.ArgumentParser()
    p.add_argument("--h5ad_path", required=True)
    p.add_argument("--min_cells", type=int, default=5)
    p.add_argument("--min_genes", type=int, default=5)
    p.add_argument("--technology", type=str, default='10X')
    p.add_argument("--normalisation_method", type=str, default='logCP10K')
    p.add_argument("--aggregation_min_cells", type=int, default=5)
    p.add_argument("--aggregation_method", type=str, default='mean', help="Aggregation method(s) to use, e.g. 'mean' or 'mean,sum'")
    p.add_argument("--datasource_id", type=str, required=True)
    p.add_argument("--datatype_id", type=str, default="scrna-seq")
    p.add_argument("--biosample_index_path", type=str, required=True)
    p.add_argument("--donor_colname", type=str, default='donor_id')
    p.add_argument("--tissue_agg_colname", type=str, default='tissue_ontology_term_id')
    p.add_argument("--celltype_agg_colname", type=str, default='cell_type_ontology_term_id')
    p.add_argument("--age_colname", type=str, default='age')
    p.add_argument("--sex_colname", type=str, default='sex')
    p.add_argument("--ethnicity_colname", type=str, default='ethnicity')

    args = p.parse_args()
    methods = args.aggregation_method.split(',') 
    
    PseudobulkExpression(
        args.h5ad_path,
        datasource_id = args.datasource_id,
        datatype_id   = args.datatype_id,
        unit          = args.normalisation_method,
        biosample_index_path = args.biosample_index_path,
    ).main(args, methods)

# For example usage, uncomment the following lines:
# agg_methods = ['mean']
# pe = PseudobulkExpression(
#     '/home/alegbe/data/tabula_sapiens-small_intestine-CxG.h5ad',
#     datasource_id = 'tabula_sapiens',
#     datatype_id   = 'scrna-seq',
#     unit          = 'logCP10K',
#     biosample_index_path = 'gs://open-targets-data-releases/25.06/output/biosample'
# )
# pe.read_h5ad()
# pe.filter_anndata(min_cells=5, min_genes=0, technology='10X')
# pe.normalise_anndata(method='logCP10K')
# pe.pseudobulk_data(
#     donor_colname='donor_id',
#     min_cells=5,
#     tissue_agg_colname='tissue_ontology_term_id',
#     agg_method='mean'
# )
