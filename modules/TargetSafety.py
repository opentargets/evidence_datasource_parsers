#!/usr/bin/env python
"""This module puts together data from different sources that describe target safety liabilities."""

import argparse
from functools import partial, reduce
import logging

from pyspark import SparkFiles
from pyspark.sql import DataFrame, Column, SparkSession
import pyspark.sql.functions as f
from pyspark.sql.types import ArrayType, StructType

from common.evidence import initialize_logger, initialize_sparksession, write_evidence_strings
from common.ontology import add_efo_mapping


def main(
    spark: SparkSession,
    toxcast: str,
    adverse_events: str,
    safety_risk: str,
    pharmacogenetics: str,
    aopwiki: str,
    brennan: str,
    output: str,
    cache_dir: str,
):
    """
    This module puts together data from different sources that describe target safety liabilities.

    Args:
        adverse_events: Input TSV containing adverse events associated with targets that have been collected from relevant publications. Fetched from GitHub.
        safety_risk: Input TSV containing cardiovascular safety liabilities associated with targets that have been collected from relevant publications. Fetched from GitHub.
        toxcast: Input table containing biological processes associated with relevant targets that have been observed in toxicity assays.
        pharmacogenetics: Input JSON containing the toxicogenomic evidence curated by PharmGKB.
        brennan: Input JSON containing curated evidence of secondary pharmacology extracted from Table 1 of 38773351
        output: Output gzipped json file following the target safety liabilities data model.
        log_file: Destination of the logs generated by this script. Defaults to None.
    """

    # Logger initializer. If no log_file is specified, logs are written to stderr
    logger = logging.getLogger(__name__)
    spark.sparkContext.addFile(adverse_events)
    spark.sparkContext.addFile(safety_risk)
    logger.info("Remote files successfully added to the Spark Context.")

    safety_dfs = [
        process_adverse_events(SparkFiles.get(adverse_events.split("/")[-1])),
        process_safety_risk(SparkFiles.get(safety_risk.split("/")[-1])),
        process_toxcast(toxcast),
        process_aop(aopwiki),
        process_pharmacogenetics(spark, spark.read.json(pharmacogenetics), cache_dir),
        process_brennan(spark.read.json(brennan)),
    ]
    logger.info("Data has been processed. Merging...")

    # Combine dfs and group evidence
    evidence_unique_cols = [
        "targetFromSourceId",
        "event",
        "eventId",
        "datasource",
        "effects",
        "literature",
        "url",
    ]
    unionByDiffSchema = partial(DataFrame.unionByName, allowMissingColumns=True)
    safety_df = (
        reduce(unionByDiffSchema, safety_dfs)
        # Collect biosample and study metadata by grouping on the unique evidence fields
        .groupBy(evidence_unique_cols)
        .agg(
            f.collect_set(f.col("biosample")).alias("biosamples"),
            f.collect_set(f.col("study")).alias("studies"),
            f.collect_set(f.col("supporting_variation")).alias("supporting_variation"),
        )
        .withColumn(
            "biosamples", f.when(f.size("biosamples") != 0, f.col("biosamples"))
        )
        # Add the supporting variation to the study metadata for the PGx evidence
        .withColumn(
            "studies",
            f.when(
                f.col("datasource") == "PharmGKB",
                f.transform(
                    f.col("studies"),
                    lambda x: f.struct(
                        f.concat(
                            f.lit("Genetic variation linked to this safety liability: "), f.array_join(f.col("supporting_variation"), ", ")
                        ).alias("description"),
                        x["name"].alias("name"),
                        x["type"].alias("type"),
                    ),
                )
            ).otherwise(f.col("studies")),
        )
        .withColumn("studies", f.when(f.size("studies") != 0, f.col("studies")))
        .withColumnRenamed("targetFromSourceId", "id")
        .drop("supporting_variation")
        .distinct()
    )

    # Write output
    logger.info("Evidence strings have been processed. Saving...")
    write_evidence_strings(safety_df, output)
    logger.info(
        f"{safety_df.count()} evidence of safety liabilities have been saved to {output}. Exiting."
    )

    return 0


def process_aop(aopwiki: str) -> DataFrame:
    """Loads and processes the AOPWiki input JSON."""

    return (
        spark.read.json(aopwiki)
        .withColumnRenamed("id", "targetFromSourceId")
        # data bug: some events have the substring "NA" at the start - removal and trim the string
        .withColumn("event", f.trim(f.regexp_replace(f.col("event"), "^NA", "")))
        # data bug: effects.direction need to be in lowercase, this field is an enum
        .withColumn(
            "effects",
            f.transform(
                f.col("effects"),
                lambda x: f.struct(
                    f.when(
                        x.direction == "Activation",
                        f.lit("Activation/Increase/Upregulation"),
                    )
                    .when(
                        x.direction == "Inhibition",
                        f.lit("Inhibition/Decrease/Downregulation"),
                    )
                    .alias("direction"),
                    x.dosing.alias("dosing"),
                ),
            ),
        )
        # I need to convert the biosamples array into a struct so that data is parsed the same way as the rest of the sources
        .withColumn("biosample", f.explode_outer("biosamples"))
    )


def process_adverse_events(adverse_events: str) -> DataFrame:
    """
    Loads and processes the adverse events input TSV.

    Ex. input record:
        biologicalSystem | gastrointestinal
        effect           | activation_general
        efoId            | EFO_0009836
        ensemblId        | ENSG00000133019
        pmid             | 23197038
        ref              | Bowes et al. (2012)
        symptom          | bronchoconstriction
        target           | CHRM3
        uberonCode       | UBERON_0005409
        url              | null

    Ex. output record:
        targetFromSourceId    | ENSG00000133019
        event                 | bronchoconstriction
        datasource            | Bowes et al. (2012)
        eventId               | EFO_0009836
        literature            | 23197038
        url                   | null
        biosample             | {gastrointestinal, UBERON_0005409, null, null, null}
        effects               | [{Activation/Increase/Upregulation, general}]
    """

    ae_df = (
        spark.read.csv(adverse_events, sep="\t", header=True)
        .select(
            f.col("ensemblId").alias("targetFromSourceId"),
            f.col("symptom").alias("event"),
            f.col("efoId").alias("eventId"),
            f.col("ref").alias("datasource"),
            f.col("pmid").alias("literature"),
            "url",
            f.struct(
                f.col("biologicalSystem").alias("tissueLabel"),
                f.col("uberonCode").alias("tissueId"),
                f.lit(None).alias("cellLabel"),
                f.lit(None).alias("cellFormat"),
                f.lit(None).alias("cellId"),
            ).alias("biosample"),
            f.split(f.col("effect"), "_").alias("effects"),
        )
        .withColumn(
            "effects",
            f.struct(
                f.when(
                    f.col("effects")[0].contains("activation"),
                    f.lit("Activation/Increase/Upregulation"),
                )
                .when(
                    f.col("effects")[0].contains("inhibition"),
                    f.lit("Inhibition/Decrease/Downregulation"),
                )
                .alias("direction"),
                f.element_at(f.col("effects"), 2).alias("dosing"),
            ),
        )
    )

    # Multiple dosing effects need to be grouped in the same record.
    effects_df = ae_df.groupBy("targetFromSourceId", "event", "datasource").agg(
        f.collect_set(f.col("effects")).alias("effects")
    )
    ae_df = ae_df.drop("effects").join(
        effects_df, on=["targetFromSourceId", "event", "datasource"], how="left"
    )

    return ae_df

def process_brennan(brennan_df: DataFrame) -> DataFrame:
    """Loads and processes the Brennan input JSON prepared by the Target Safety team."""

    return (
        brennan_df
        .withColumn(
            "effects",
            f.array(
                f.struct(
                    f.when(
                    f.col("effects.direction") == "Activation",
                    f.lit("Activation/Increase/Upregulation")
                    ).when(
                        f.col("effects.direction") == "Inhibition",
                        f.lit("Inhibition/Decrease/Downregulation")
                    )
                    .otherwise(f.col("effects.direction"))
                    .alias("direction"),
                    f.col("effects.dosing")
                )
            )
        )
        .withColumnRenamed("studies", "study")
        .withColumnRenamed("biosamples", "biosample")
        .withColumnRenamed("id", "targetFromSourceId")
        .drop("Type")
    )

def process_safety_risk(safety_risk: str) -> DataFrame:
    """
    Loads and processes the safety risk information input TSV.

    Ex. input record:
        biologicalSystem | cardiovascular sy...
        ensemblId        | ENSG00000132155
        event            | heart disease
        eventId          | EFO_0003777
        liability        | Important for the...
        pmid             | 21283106
        ref              | Force et al. (2011)
        target           | RAF1
        uberonId         | UBERON_0004535

    Ex. output record:
        id         | ENSG00000132155
        event      | heart disease
        eventId    | EFO_0003777
        literature | 21283106
        datasource | Force et al. (2011)
        biosample  | {cardiovascular s...
        study      | {Important for th...
    """

    return (
        spark.read.csv(safety_risk, sep="\t", header=True)
        .select(
            f.col("ensemblId").alias("targetFromSourceId"),
            "event",
            "eventId",
            f.col("pmid").alias("literature"),
            f.col("ref").alias("datasource"),
            f.struct(
                f.col("biologicalSystem").alias("tissueLabel"),
                f.col("uberonId").alias("tissueId"),
                f.lit(None).alias("cellLabel"),
                f.lit(None).alias("cellFormat"),
                f.lit(None).alias("cellId"),
            ).alias("biosample"),
            f.struct(
                f.col("liability").alias("description"),
                f.lit(None).alias("name"),
                f.lit(None).alias("type"),
            ).alias("study"),
        )
        .withColumn(
            "event",
            f.when(f.col("datasource").contains("Force"), "heart disease").when(
                f.col("datasource").contains("Lamore"), "cardiac arrhythmia"
            ),
        )
        .withColumn(
            "eventId",
            f.when(f.col("datasource").contains("Force"), "EFO_0003777").when(
                f.col("datasource").contains("Lamore"), "EFO_0004269"
            ),
        )
    )


def process_toxcast(toxcast: str) -> DataFrame:
    """
    Loads and processes the ToxCast input table.

    Ex. input record:
        assay_component_endpoint_name | ACEA_ER_80hr
        assay_component_desc          | ACEA_ER_80hr, is ...
        biological_process_target     | cell proliferation
        tissue                        | null
        cell_format                   | cell line
        cell_short_name               | T47D
        assay_format_type             | cell-based
        official_symbol               | ESR1
        eventId                       | null

    Ex. output record:
    targetFromSourceId | ESR1
    event              | cell proliferation
    eventId            | null
    biosample          | {null, null, T47D...
    datasource         | ToxCast
    url                | https://www.epa.g...
    study              | {ACEA_ER_80hr, AC...
    """

    return spark.read.csv(toxcast, sep="\t", header=True).select(
        f.trim(f.col("official_symbol")).alias("targetFromSourceId"),
        f.col("biological_process_target").alias("event"),
        "eventId",
        f.struct(
            f.col("tissue").alias("tissueLabel"),
            f.lit(None).alias("tissueId"),
            f.col("cell_short_name").alias("cellLabel"),
            f.col("cell_format").alias("cellFormat"),
            f.lit(None).alias("cellId"),
        ).alias("biosample"),
        f.lit("ToxCast").alias("datasource"),
        f.lit(
            "https://www.epa.gov/chemical-research/exploring-toxcast-data-downloadable-data"
        ).alias("url"),
        f.struct(
            f.col("assay_component_endpoint_name").alias("name"),
            f.col("assay_component_desc").alias("description"),
            f.col("assay_format_type").alias("type"),
        ).alias("study"),
    )

def process_pharmacogenetics(spark: SparkSession, pgx_df: DataFrame, cache_dir: str) -> DataFrame:
    """Given the pharmacogenetics evidence, extract the evidence related to target toxicity."""
    pgkb_url_template = "https://www.pharmgkb.org/search?query="
    pgx_safety = (
        pgx_df
        # Only interested in the evidence that is related to toxicity
        .filter(f.col("pgxCategory") == "toxicity")
        .filter((f.col("targetFromSourceId").isNotNull()) & (f.col("phenotypeText").isNotNull()))
        # Safety liabilities extraction
        .filter(~(f.col("phenotypeText").contains("no significant association") | f.col("phenotypeText").contains("not associated with")))
        .withColumn("event", clean_phenotype_to_describe_safety_event(f.col("phenotypeText")))
        .filter(f.col("event") != "drug response")
        # Explode evidence by the variation that supports the liability - this will be used to build the study metadata
        .withColumn("supporting_variation", f.explode(
                f.array_union(f.array("genotypeId"), f.array("haplotypeId")),
            )
        )
        # Define unaggregated target/event pairs
        .select(
            f.col("targetFromSourceId"),
            "event",
            f.lit("PharmGKB").alias("datasource"),
            f.concat(f.lit(pgkb_url_template), f.col("targetFromSourceId")).alias("url"),
            # To build study metadata later - each study is a drug after which the phenotype was observed
            f.explode(f.col("drugs.drugFromSource")).alias("drugFromSource"),
            "supporting_variation",
        )
        .withColumn(
            "study", f.struct(
                f.concat(f.col("drugFromSource"), f.lit(" induced effect")).alias("name"),
                f.lit("patient-level").alias("type"),
            )
        )
    )
    return add_efo_mapping(
        pgx_safety.select("*", f.col("event").alias("diseaseFromSource"), f.lit(None).alias("diseaseFromSourceId")),
        spark,
        cache_dir
    ).withColumnRenamed("diseaseFromSourceMappedId", "eventId").drop("diseaseFromSource")


def clean_phenotype_to_describe_safety_event(phenotype_col_name: Column) -> Column:
    words_to_remove = (
        r" but not absent,|but not absent|, but not absent,|, but not absent|, but not non-existent,|(but not absent) |improved|risk and severity of|increased risk and increased severity of|reduced risk and reduced severity of|decreased risk and reduced severity of|similar|increased|decreased|not have altered risk of |greater|reduced|phenotype|risk of|risk for|risk of developing|less likely to |more likely to |less severe|more severe|smaller|likelihood of|severity of|developing|experiencing|experience|develop|treatment related|drug-induced|oxcarbazepine-induced|aspirin induced|higher|lower|INCREASED|Increased|including|drug toxicity, particularly |unknown likelihood of experiencing|high frequency of"
    )
    context_stop_words = (
        r"(as a result of taking|based on|when administered|during treatment|when taking|due to unintentional|with docetaxel and thalidomide)"
    )
    pattern = fr"^(.*?)(?:{context_stop_words}.*)?$"
    replacements = {
        "toxicity": "drug toxicity",
        "response": "drug response",
        "altered drug toxicity": "drug toxicity",
        "risk and drug toxicity": "drug toxicity",
        "risk": "drug toxicity"
    }

    cleaned_col = f.regexp_replace(phenotype_col_name, words_to_remove, "")
    cleaned_col = f.regexp_extract(cleaned_col, pattern, 1)
    cleaned_col = f.trim(f.regexp_replace(cleaned_col, "\\s+", " "))
  
    for original, replacement in replacements.items():
        cleaned_col = f.when(cleaned_col == original, f.lit(replacement)).otherwise(cleaned_col)

    return cleaned_col


def get_parser():
    """Get parser object for script TargetSafety.py."""
    parser = argparse.ArgumentParser(description=__doc__)

    parser.add_argument(
        "--toxcast",
        help="Input table containing biological processes associated with relevant targets that have been observed in toxicity assays.",
        type=str,
        required=True,
    )
    parser.add_argument(
        "--adverse_events",
        help="Input TSV containing adverse events associated with targets that have been collected from relevant publications. Fetched from https://raw.githubusercontent.com/opentargets/curation/master/target_safety/adverse_effects.tsv.",
        type=str,
        required=True,
    )
    parser.add_argument(
        "--safety_risk",
        help="Input TSV containing cardiovascular safety liabilities associated with targets that have been collected from relevant publications. Fetched from https://raw.githubusercontent.com/opentargets/curation/master/target_safety/safety_risks.tsv.",
        type=str,
        required=True,
    )
    parser.add_argument(
        "--aopwiki",
        help="Input JSON containing targets implicated in adverse outcomes as reported by the AOPWiki. Parsed from their source XML data.",
        type=str,
        required=True,
    )
    parser.add_argument(
        "--pharmacogenetics",
        help="Input JSON containing the toxicogenomic evidence curated by PharmGKB.",
        type=str,
        required=True,
    )
    parser.add_argument(
        "--brennan",
        help="Input JSON containing curated evidence of secondary pharmacology extracted from Table 1 of 38773351.",
        type=str,
        required=True,
    )
    parser.add_argument(
        "--output",
        help="Output gzipped json file following the target safety liabilities data model.",
        type=str,
        required=True,
    )
    parser.add_argument(
        "--log_file",
        help="Destination of the logs generated by this script. Defaults to None",
        type=str,
        nargs="?",
        default=None,
    )
    parser.add_argument(
        "--cache_dir",
        required=False,
        help="Directory to store the OnToma cache files in.",
    )

    return parser


if __name__ == "__main__":
    args = get_parser().parse_args()

    spark = initialize_sparksession()
    initialize_logger(__name__, args.log_file)
    main(
        spark,
        toxcast=args.toxcast,
        adverse_events=args.adverse_events,
        safety_risk=args.safety_risk,
        aopwiki=args.aopwiki,
        pharmacogenetics=args.pharmacogenetics,
        brennan=args.brennan,
        cache_dir=args.cache_dir,
        output=args.output,
    )
