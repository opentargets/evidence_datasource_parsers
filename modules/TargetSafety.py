#!/usr/bin/env python
"""This module puts together data from different sources that describe target safety liabilities."""

import argparse
import logging
from functools import partial, reduce

import pyspark.sql.functions as f
from common.evidence import (
    initialize_logger,
    initialize_sparksession,
    write_evidence_strings,
)
from common.ontology import add_efo_mapping
from pyspark import SparkFiles
from pyspark.sql import Column, DataFrame, SparkSession


def main(
    spark: SparkSession,
    toxcast: str,
    adverse_events: str,
    safety_risk: str,
    pharmacogenetics: str,
    aopwiki: str,
    brennan: str,
    output: str,
    cache_dir: str,
):
    """This module puts together data from different sources that describe target safety liabilities.

    Args:
        adverse_events: Input TSV containing adverse events associated with targets that have been collected from relevant publications. Fetched from GitHub.
        safety_risk: Input TSV containing cardiovascular safety liabilities associated with targets that have been collected from relevant publications. Fetched from GitHub.
        toxcast: Input table containing biological processes associated with relevant targets that have been observed in toxicity assays.
        pharmacogenetics: Input JSON containing the toxicogenomic evidence curated by PharmGKB.
        brennan: Input JSON containing curated evidence of secondary pharmacology extracted from Table 1 of 38773351
        output: Output gzipped json file following the target safety liabilities data model.
        log_file: Destination of the logs generated by this script. Defaults to None.
    """
    # Logger initializer. If no log_file is specified, logs are written to stderr
    logger = logging.getLogger(__name__)
    spark.sparkContext.addFile(adverse_events)
    spark.sparkContext.addFile(safety_risk)
    logger.info("Remote files successfully added to the Spark Context.")

    safety_dfs = [
        process_adverse_events(SparkFiles.get(adverse_events.split("/")[-1])),
        process_safety_risk(SparkFiles.get(safety_risk.split("/")[-1])),
        process_toxcast(toxcast),
        process_aop(aopwiki),
        process_pharmacogenetics(spark, spark.read.json(pharmacogenetics), cache_dir),
        process_brennan(spark.read.json(brennan)),
    ]
    logger.info("Data has been processed. Merging...")

    # Combine dfs and group evidence
    evidence_unique_cols = [
        "id",
        "targetFromSourceId",
        "event",
        "eventId",
        "datasource",
        "effects",
        "literature",
        "url",
    ]
    unionByDiffSchema = partial(DataFrame.unionByName, allowMissingColumns=True)
    safety_df = (
        reduce(unionByDiffSchema, safety_dfs)
        # Collect biosample and study metadata by grouping on the unique evidence fields
        .groupBy(evidence_unique_cols)
        .agg(
            f.collect_set(f.col("biosample")).alias("biosamples"),
            f.collect_set(f.col("study")).alias("studies"),
            f.collect_set(f.col("supporting_variation")).alias("supporting_variation"),
        )
        .withColumn(
            "biosamples", f.when(f.size("biosamples") != 0, f.col("biosamples"))
        )
        # Add the supporting variation to the study metadata for the PGx evidence
        .withColumn(
            "studies",
            f.when(
                f.col("datasource") == "PharmGKB",
                f.transform(
                    f.col("studies"),
                    lambda x: f.struct(
                        f.concat(
                            f.lit(
                                "Genetic variation linked to this safety liability: "
                            ),
                            f.array_join(f.col("supporting_variation"), ", "),
                        ).alias("description"),
                        x["name"].alias("name"),
                        x["type"].alias("type"),
                    ),
                ),
            ).otherwise(f.col("studies")),
        )
        .withColumn("studies", f.when(f.size("studies") != 0, f.col("studies")))
        .drop("supporting_variation")
        .distinct()
    )

    # Write output
    logger.info("Evidence strings have been processed. Saving...")
    write_evidence_strings(safety_df, output)
    logger.info(
        f"{safety_df.count()} evidence of safety liabilities have been saved to {output}. Exiting."
    )

    return 0


def process_aop(aopwiki: str) -> DataFrame:
    """Loads and processes the AOPWiki input JSON."""
    return (
        spark.read.json(aopwiki)
        # data bug: some events have the substring "NA" at the start - removal and trim the string
        .withColumn("event", f.trim(f.regexp_replace(f.col("event"), "^NA", "")))
        # data bug: effects.direction need to be in lowercase, this field is an enum
        .withColumn(
            "effects",
            f.transform(
                f.col("effects"),
                lambda x: f.struct(
                    f.when(
                        x.direction == "Activation",
                        f.lit("Activation/Increase/Upregulation"),
                    )
                    .when(
                        x.direction == "Inhibition",
                        f.lit("Inhibition/Decrease/Downregulation"),
                    )
                    .alias("direction"),
                    x.dosing.alias("dosing"),
                ),
            ),
        )
        # I need to convert the biosamples array into a struct so that data is parsed the same way as the rest of the sources
        .withColumn("biosample", f.explode_outer("biosamples"))
    )


def process_adverse_events(adverse_events: str) -> DataFrame:
    """Loads and processes the adverse events input TSV.

    Ex. input record:
        biologicalSystem | gastrointestinal
        effect           | activation_general
        efoId            | EFO_0009836
        ensemblId        | ENSG00000133019
        pmid             | 23197038
        ref              | Bowes et al. (2012)
        symptom          | bronchoconstriction
        target           | CHRM3
        uberonCode       | UBERON_0005409
        url              | null

    Ex. output record:
        id          | ENSG00000133019
        event       | bronchoconstriction
        datasource  | Bowes et al. (2012)
        eventId     | EFO_0009836
        literature  | 23197038
        url         | null
        biosample   | {gastrointestinal, UBERON_0005409, null, null, null}
        effects     | [{Activation/Increase/Upregulation, general}]
    """
    ae_df = (
        spark.read.csv(adverse_events, sep="\t", header=True)
        .select(
            f.col("ensemblId").alias("id"),
            f.col("symptom").alias("event"),
            f.col("efoId").alias("eventId"),
            f.col("ref").alias("datasource"),
            f.col("pmid").alias("literature"),
            "url",
            f.struct(
                f.col("biologicalSystem").alias("tissueLabel"),
                f.col("uberonCode").alias("tissueId"),
                f.lit(None).alias("cellLabel"),
                f.lit(None).alias("cellFormat"),
                f.lit(None).alias("cellId"),
            ).alias("biosample"),
            f.split(f.col("effect"), "_").alias("effects"),
        )
        .withColumn(
            "effects",
            f.struct(
                f.when(
                    f.col("effects")[0].contains("activation"),
                    f.lit("Activation/Increase/Upregulation"),
                )
                .when(
                    f.col("effects")[0].contains("inhibition"),
                    f.lit("Inhibition/Decrease/Downregulation"),
                )
                .alias("direction"),
                f.element_at(f.col("effects"), 2).alias("dosing"),
            ),
        )
        .withColumn("studies", f.array(
            f.struct(
                f.col("event").alias("description"),
                f.col("eventId").alias("name"),
                f.lit("preclinical").alias("type"),
            )
        ))
    )

    # Multiple dosing effects need to be grouped in the same record.
    effects_df = ae_df.groupBy("id", "event", "datasource").agg(
        f.collect_set(f.col("effects")).alias("effects")
    )
    ae_df = ae_df.drop("effects").join(
        effects_df, on=["id", "event", "datasource"], how="left"
    )

    return ae_df


def process_brennan(brennan_df: DataFrame) -> DataFrame:
    """Loads and processes the Brennan input JSON prepared by the Target Safety team."""
    return (
        brennan_df.withColumn(
            "effects",
            f.array(
                f.struct(
                    f.when(
                        f.col("effects.direction") == "Activation",
                        f.lit("Activation/Increase/Upregulation"),
                    )
                    .when(
                        f.col("effects.direction") == "Inhibition",
                        f.lit("Inhibition/Decrease/Downregulation"),
                    )
                    .otherwise(f.col("effects.direction"))
                    .alias("direction"),
                    f.col("effects.dosing"),
                )
            ),
        )
        .withColumnRenamed("studies", "study")
        .withColumnRenamed("biosamples", "biosample")
        .drop("Type")
    )


def process_safety_risk(safety_risk: str) -> DataFrame:
    """Loads and processes the safety risk information input TSV.

    Ex. input record:
        biologicalSystem | cardiovascular sy...
        ensemblId        | ENSG00000132155
        event            | heart disease
        eventId          | EFO_0003777
        liability        | Important for the...
        pmid             | 21283106
        ref              | Force et al. (2011)
        target           | RAF1
        uberonId         | UBERON_0004535

    Ex. output record:
        id         | ENSG00000132155
        event      | heart disease
        eventId    | EFO_0003777
        literature | 21283106
        datasource | Force et al. (2011)
        biosample  | {cardiovascular s...
        study      | {Important for th...
    """
    return (
        spark.read.csv(safety_risk, sep="\t", header=True)
        .withColumn(
            "studyType",
            f.when(f.col("datasource").contains("Force"), "preclinical").when(
                f.col("datasource").contains("Lamore"), "cell-based"
            ),
        )
        .select(
            f.col("ensemblId").alias("id"),
            "event",
            "eventId",
            f.col("pmid").alias("literature"),
            f.col("ref").alias("datasource"),
            f.struct(
                f.col("biologicalSystem").alias("tissueLabel"),
                f.col("uberonId").alias("tissueId"),
                f.lit(None).alias("cellLabel"),
                f.lit(None).alias("cellFormat"),
                f.lit(None).alias("cellId"),
            ).alias("biosample"),
            f.struct(
                f.col("liability").alias("description"),
                f.lit(None).alias("name"),
                f.col("studyType").alias("type"),
            ).alias("study"),
        )
        .withColumn(
            "event",
            f.when(f.col("datasource").contains("Force"), "heart disease").when(
                f.col("datasource").contains("Lamore"), "cardiac arrhythmia"
            ),
        )
        .withColumn(
            "eventId",
            f.when(f.col("datasource").contains("Force"), "EFO_0003777").when(
                f.col("datasource").contains("Lamore"), "EFO_0004269"
            ),
        )
    )


def process_toxcast(toxcast: str) -> DataFrame:
    """Loads and processes the ToxCast input table.

    Ex. input record:
        assay_component_endpoint_name | ACEA_ER_80hr
        assay_component_desc          | ACEA_ER_80hr, is ...
        biological_process_target     | cell proliferation
        tissue                        | null
        cell_format                   | cell line
        cell_short_name               | T47D
        assay_format_type             | cell-based
        official_symbol               | ESR1
        eventId                       | null

    Ex. output record:
    targetFromSourceId | ESR1
    event              | cell proliferation
    eventId            | null
    biosample          | {null, null, T47D...
    datasource         | ToxCast
    url                | https://www.epa.g...
    study              | {ACEA_ER_80hr, AC...
    """
    return spark.read.csv(toxcast, sep="\t", header=True).select(
        f.trim(f.col("official_symbol")).alias("targetFromSourceId"),
        f.col("biological_process_target").alias("event"),
        "eventId",
        f.struct(
            f.col("tissue").alias("tissueLabel"),
            f.lit(None).alias("tissueId"),
            f.col("cell_short_name").alias("cellLabel"),
            f.col("cell_format").alias("cellFormat"),
            f.lit(None).alias("cellId"),
        ).alias("biosample"),
        f.lit("ToxCast").alias("datasource"),
        f.lit(
            "https://www.epa.gov/chemical-research/exploring-toxcast-data-downloadable-data"
        ).alias("url"),
        f.struct(
            f.col("assay_component_endpoint_name").alias("name"),
            f.col("assay_component_desc").alias("description"),
            f.col("assay_format_type").alias("type"),
        ).alias("study"),
    )


def process_pharmacogenetics(
    spark: SparkSession, pgx_df: DataFrame, cache_dir: str
) -> DataFrame:
    """Given the pharmacogenetics evidence, extract the evidence related to target toxicity."""
    pgkb_url_template = "https://www.pharmgkb.org/search?query="
    pgx_safety = (
        pgx_df
        # Only interested in the evidence that is related to toxicity
        .filter(f.col("pgxCategory") == "toxicity")
        .filter(
            (f.col("targetFromSourceId").isNotNull())
            & (f.col("phenotypeText").isNotNull())
        )
        # Safety liabilities extraction
        .filter(
            ~(
                f.col("phenotypeText").contains("no significant association")
                | f.col("phenotypeText").contains("not associated with")
            )
        )
        .withColumn(
            "event", clean_phenotype_to_describe_safety_event(f.col("phenotypeText"))
        )
        .filter(f.col("event") != "drug response")
        # Explode evidence by the variation that supports the liability - this will be used to build the study metadata
        .withColumn(
            "supporting_variation",
            f.explode(
                f.array_union(f.array("genotypeId"), f.array("haplotypeId")),
            ),
        )
        # Define unaggregated target/event pairs
        .select(
            f.col("targetFromSourceId").alias("id"),
            "event",
            f.lit("PharmGKB").alias("datasource"),
            f.concat(f.lit(pgkb_url_template), f.col("targetFromSourceId")).alias(
                "url"
            ),
            # To build study metadata later - each study is a drug after which the phenotype was observed
            f.explode(f.col("drugs.drugFromSource")).alias("drugFromSource"),
            "supporting_variation",
        )
        .withColumn(
            "study",
            f.struct(
                f.concat(f.col("drugFromSource"), f.lit(" induced effect")).alias(
                    "name"
                ),
                f.lit("clinical").alias("type"),
            ),
        )
    )
    return (
        add_efo_mapping(
            pgx_safety.select(
                "*",
                f.col("event").alias("diseaseFromSource"),
                f.lit(None).alias("diseaseFromSourceId"),
            ),
            spark,
            cache_dir,
        )
        .withColumnRenamed("diseaseFromSourceMappedId", "eventId")
        .drop("diseaseFromSource")
    )


def clean_phenotype_to_describe_safety_event(phenotype_col_name: Column) -> Column:
    words_to_remove = r" but not absent,|but not absent|, but not absent,|, but not absent|, but not non-existent,|(but not absent) |improved|risk and severity of|increased risk and increased severity of|reduced risk and reduced severity of|decreased risk and reduced severity of|similar|increased|decreased|not have altered risk of |greater|reduced|phenotype|risk of|risk for|risk of developing|less likely to |more likely to |less severe|more severe|smaller|likelihood of|severity of|developing|experiencing|experience|develop|treatment related|drug-induced|oxcarbazepine-induced|aspirin induced|higher|lower|INCREASED|Increased|including|drug toxicity, particularly |unknown likelihood of experiencing|high frequency of"
    context_stop_words = r"(as a result of taking|based on|when administered|during treatment|when taking|due to unintentional|with docetaxel and thalidomide)"
    pattern = rf"^(.*?)(?:{context_stop_words}.*)?$"
    replacements = {
        "toxicity": "drug toxicity",
        "response": "drug response",
        "altered drug toxicity": "drug toxicity",
        "risk and drug toxicity": "drug toxicity",
        "risk": "drug toxicity",
    }

    cleaned_col = f.regexp_replace(phenotype_col_name, words_to_remove, "")
    cleaned_col = f.regexp_extract(cleaned_col, pattern, 1)
    cleaned_col = f.trim(f.regexp_replace(cleaned_col, "\\s+", " "))

    for original, replacement in replacements.items():
        cleaned_col = f.when(cleaned_col == original, f.lit(replacement)).otherwise(
            cleaned_col
        )

    return cleaned_col


def get_parser():
    """Get parser object for script TargetSafety.py."""
    parser = argparse.ArgumentParser(description=__doc__)

    parser.add_argument(
        "--toxcast",
        help="Input table containing biological processes associated with relevant targets that have been observed in toxicity assays.",
        type=str,
        required=True,
    )
    parser.add_argument(
        "--adverse_events",
        help="Input TSV containing adverse events associated with targets that have been collected from relevant publications. Fetched from https://raw.githubusercontent.com/opentargets/curation/master/target_safety/adverse_effects.tsv.",
        type=str,
        required=True,
    )
    parser.add_argument(
        "--safety_risk",
        help="Input TSV containing cardiovascular safety liabilities associated with targets that have been collected from relevant publications. Fetched from https://raw.githubusercontent.com/opentargets/curation/master/target_safety/safety_risks.tsv.",
        type=str,
        required=True,
    )
    parser.add_argument(
        "--aopwiki",
        help="Input JSON containing targets implicated in adverse outcomes as reported by the AOPWiki. Parsed from their source XML data.",
        type=str,
        required=True,
    )
    parser.add_argument(
        "--pharmacogenetics",
        help="Input JSON containing the toxicogenomic evidence curated by PharmGKB.",
        type=str,
        required=True,
    )
    parser.add_argument(
        "--brennan",
        help="Input JSON containing curated evidence of secondary pharmacology extracted from Table 1 of 38773351.",
        type=str,
        required=True,
    )
    parser.add_argument(
        "--output",
        help="Output gzipped json file following the target safety liabilities data model.",
        type=str,
        required=True,
    )
    parser.add_argument(
        "--log_file",
        help="Destination of the logs generated by this script. Defaults to None",
        type=str,
        nargs="?",
        default=None,
    )
    parser.add_argument(
        "--cache_dir",
        required=False,
        help="Directory to store the OnToma cache files in.",
    )

    return parser


if __name__ == "__main__":
    args = get_parser().parse_args()

    spark = initialize_sparksession()
    initialize_logger(__name__, args.log_file)
    main(
        spark,
        toxcast=args.toxcast,
        adverse_events=args.adverse_events,
        safety_risk=args.safety_risk,
        aopwiki=args.aopwiki,
        pharmacogenetics=args.pharmacogenetics,
        brennan=args.brennan,
        cache_dir=args.cache_dir,
        output=args.output,
    )
