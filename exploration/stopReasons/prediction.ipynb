{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise Spark\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP\")\\\n",
    "    .master(\"local[4]\")\\\n",
    "    .config(\"spark.driver.memory\",\"16G\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"0\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"2000M\")\\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:3.3.4\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT fine tuned model\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    \"\"\"Bert Model for Classification Tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, freeze_bert=False):\n",
    "        \"\"\"\n",
    "        @param    bert: a BertModel object\n",
    "        @param    classifier: a torch.nn.Module classifier\n",
    "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
    "        \"\"\"\n",
    "        super(BertClassifier, self).__init__()\n",
    "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
    "        D_in, H, D_out = 768, 50, 17\n",
    "\n",
    "        # Instantiate BERT model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # Instantiate an one-layer feed-forward classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.5),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "\n",
    "        # Freeze the BERT model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Feed input to BERT and the classifier to compute logits.\n",
    "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
    "                      max_length)\n",
    "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
    "                      information with shape (batch_size, max_length)\n",
    "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
    "                      num_labels)\n",
    "        \"\"\"\n",
    "        # Feed input to BERT\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        \n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "\n",
    "        # Feed input to classifier to compute logits\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits\n",
    "\n",
    "model = torch.load('model/bert_trials.pth')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "CHEMBL_EVIDENCE_PATH = 'data/chembl-2021-08-23.json.gz'\n",
    "\n",
    "stopReasons = (\n",
    "        spark.read.json(CHEMBL_EVIDENCE_PATH)\n",
    "\n",
    "        # Extract a test set\n",
    "        .sample(0.01)\n",
    "\n",
    "        # Extract studies with their reasons to stop\n",
    "        .withColumn('urls', F.explode('urls'))\n",
    "        .filter(F.col('urls.niceName').contains('ClinicalTrials'))\n",
    "        .withColumn('nct_id', F.element_at(F.split(F.col('urls.url'), '%22'), -2))\n",
    "        .select('nct_id', 'studyStopReason')\n",
    "        .filter(F.col('studyStopReason').isNotNull())\n",
    "        .distinct()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Pipeline\n",
    "\n",
    "What the Pipeline should consist of:\n",
    "- Document Assembler: converts the raw string to documents that Spark NLP can handle.\n",
    "- Tokenize each document with a series of constraints:\n",
    "  I have to reproduce this in SparkNLP's built-in tokenizer.\n",
    "  ```\n",
    "  encoded_sent = ( \n",
    "            BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "            .encode_plus(\n",
    "            text=text_preprocessing(sent),  # Preprocess sentence\n",
    "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
    "            max_length=MAX_LEN,             # Max length to truncate/pad\n",
    "            pad_to_max_length=True,         # Pad sentence to max length\n",
    "            #return_tensors='pt',           # Return PyTorch tensor\n",
    "            return_attention_mask=True      # Return attention mask\n",
    "    )\n",
    "  ```\n",
    "- Create DataLoader. Transformers is fed with 2 Tensors: input_ids (the id representation of each token) and attention masks (mask that identifies whether a token is made out of padding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = (\n",
    "    DocumentAssembler()\n",
    "    .setInputCol('studyStopReason').setOutputCol('document')\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer().setInputCols('document').setOutputCol('token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline().setStages([\n",
    "    document, tokenizer\n",
    "])\n",
    "\n",
    "model = pipeline.fit(stopReasons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(nct_id='NCT00880373', studyStopReason='The funding withdrawal and early termination of the trial is based upon lack of suitable recruitment figures in order to reach the required trial endpoints.', document=[Row(annotatorType='document', begin=0, end=155, result='The funding withdrawal and early termination of the trial is based upon lack of suitable recruitment figures in order to reach the required trial endpoints.', metadata={'sentence': '0'}, embeddings=[])], token=[Row(annotatorType='token', begin=0, end=2, result='The', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=4, end=10, result='funding', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=12, end=21, result='withdrawal', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=23, end=25, result='and', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=27, end=31, result='early', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=33, end=43, result='termination', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=45, end=46, result='of', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=48, end=50, result='the', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=52, end=56, result='trial', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=58, end=59, result='is', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=61, end=65, result='based', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=67, end=70, result='upon', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=72, end=75, result='lack', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=77, end=78, result='of', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=80, end=87, result='suitable', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=89, end=99, result='recruitment', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=101, end=107, result='figures', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=109, end=110, result='in', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=112, end=116, result='order', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=118, end=119, result='to', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=121, end=125, result='reach', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=127, end=129, result='the', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=131, end=138, result='required', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=140, end=144, result='trial', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=146, end=154, result='endpoints', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=155, end=155, result='.', metadata={'sentence': '0'}, embeddings=[])])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transform(stopReasons).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem: you cannot add custom tokenizers to the pipeline. It'll have to be more manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "def clean_sentence(sentece:str) -> str:\n",
    "    \"\"\"\n",
    "    - Remove entity mentions (eg. '@united')\n",
    "    - Correct errors (eg. '&amp;' to '&')\n",
    "    @param    text (str): a string to be processed.\n",
    "    @return   text (Str): the processed string.\n",
    "    \"\"\"\n",
    "    # Remove '@name'\n",
    "    sentece = re.sub(r'(@.*?)[\\s]', ' ', sentece)\n",
    "\n",
    "    # Replace '&amp;' with '&'\n",
    "    sentece = re.sub(r'&amp;', '&', sentece)\n",
    "\n",
    "    # Remove trailing whitespace\n",
    "    sentece = re.sub(r'\\s+', ' ', sentece).strip()\n",
    "\n",
    "    return sentece\n",
    "\n",
    "def apply_bert_tokenizer(sentence:str, bert_tokenizer, max_len:int==64):\n",
    "\n",
    "    cleaned_sentence = clean_sentence(sentence)\n",
    "\n",
    "    return ( \n",
    "        bert_tokenizer\n",
    "        .encode_plus(\n",
    "        text=cleaned_sentence,  # Preprocess sentence\n",
    "        add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
    "        max_length=max_len,             # Max length to truncate/pad\n",
    "        pad_to_max_length=True,         # Pad sentence to max length\n",
    "        return_tensors='pt',           # Return PyTorch tensor\n",
    "        return_attention_mask=True      # Return attention mask\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import HF models into Spark NLP\n",
    "\n",
    "Following https://medium.com/spark-nlp/importing-huggingface-models-into-sparknlp-8c63bdea671d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Downloading tokenizer and classification models from the Hub\n",
    "\n",
    "- Tokenizer: AutoTokenizer\n",
    "- Sentence Classificator: AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import TFRobertaModel\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, BertModel, BertTokenizer, TFRobertaModel\n",
    "\n",
    "import tensorflow\n",
    "\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = sparknlp.start()\n",
    "\n",
    "# it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"PlanTL-GOB-ES/roberta-base-biomedical-es\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"PlanTL-GOB-ES/roberta-base-biomedical-es\")\n",
    "\n",
    "# models succesfully loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can i import theses PT models from TFBertModel? No. There must be weights in TF format.\n",
    "\n",
    "# model_tf = TFRobertaModel.from_pretrained(\"PlanTL-GOB-ES/roberta-base-biomedical-es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9855023622512817,\n",
       "  'token': 3529,\n",
       "  'token_str': ' hipertensi칩n',\n",
       "  'sequence': ' El 칰nico antecedente personal a rese침ar era la hipertensi칩n arterial.'},\n",
       " {'score': 0.0039140768349170685,\n",
       "  'token': 1945,\n",
       "  'token_str': ' diabetes',\n",
       "  'sequence': ' El 칰nico antecedente personal a rese침ar era la diabetes arterial.'},\n",
       " {'score': 0.002484647324308753,\n",
       "  'token': 11483,\n",
       "  'token_str': ' hipotensi칩n',\n",
       "  'sequence': ' El 칰nico antecedente personal a rese침ar era la hipotensi칩n arterial.'},\n",
       " {'score': 0.0023484493140131235,\n",
       "  'token': 12238,\n",
       "  'token_str': ' Hipertensi칩n',\n",
       "  'sequence': ' El 칰nico antecedente personal a rese침ar era la Hipertensi칩n arterial.'},\n",
       " {'score': 0.0008009276352822781,\n",
       "  'token': 2267,\n",
       "  'token_str': ' presi칩n',\n",
       "  'sequence': ' El 칰nico antecedente personal a rese침ar era la presi칩n arterial.'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make an inference - it works!\n",
    "\n",
    "unmasker = transformers.pipeline('fill-mask', model=\"PlanTL-GOB-ES/roberta-base-biomedical-es\")\n",
    "unmasker(\"El 칰nico antecedente personal a rese침ar era la <mask> arterial.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Saving the model in Tensorflow/Pytorch format\n",
    "\n",
    "`save_pretrained` saves the model with its weights and configuration.\n",
    "\n",
    "In this case the RoBERTa model has been trained using PyTorch. We can indicate to save it in Tensorflow.\n",
    "This is also the case of Olesya's.\n",
    "\n",
    "Spark NLP needs to import a TF specific model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME_TF = 'roberta_tf'\n",
    "\n",
    "tokenizer.save_pretrained(f'model/test/{MODEL_NAME_TF}_tokenizer/')\n",
    "model.save_pretrained(f'model/test/{MODEL_NAME_TF}_classificator/', saved_model=True, save_format='tf')\n",
    "\n",
    "# models exported - but roberta_rf_classificator does not include the weights in TF format?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(f'model/test/{MODEL_NAME_TF}_classificator/', saved_model=True, save_format='tf')\n",
    "\n",
    "# There's an issue from Nov 21 saying saved_model does not work as expected. Removing the parameter doesnt work either (https://github.com/huggingface/transformers/issues/14403)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Load the model into Spark NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'roberta.embeddings.position_ids', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loaded_classificator = TFRobertaModel.from_pretrained(f'model/test/{MODEL_NAME_TF}_classificator/', from_pt=True)\n",
    "\n",
    "# Eureka!!!!!! Model can be used in Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn, pooler_layer_call_and_return_conditional_losses, embeddings_layer_call_fn while saving (showing 5 of 1050). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/roberta_tf_classificator_tf/saved_model/1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/roberta_tf_classificator_tf/saved_model/1/assets\n"
     ]
    }
   ],
   "source": [
    "# As expected, when the TF model is exported the TF structure is followed (-h5 format)\n",
    "\n",
    "loaded_classificator.save_pretrained(f'model/test/{MODEL_NAME_TF}_classificator_tf/', saved_model=True, save_format='tf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at model/test/roberta_tf_classificator_tf/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<transformers.models.roberta.modeling_tf_roberta.TFRobertaModel at 0x7fe7b5123690>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TFRobertaModel.from_pretrained(f'model/test/{MODEL_NAME_TF}_classificator_tf/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After saving the model, you also need to add the vocab.txt file to the assets directory of the saved model.\n",
    "# vocab was exported in JSON format. I have to convert it\n",
    "\n",
    "with open(f'model/test/{MODEL_NAME_TF}_tokenizer/vocab.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "with open(f'model/test/{MODEL_NAME_TF}_tokenizer/vocab.txt', 'w') as f:\n",
    "    for d in data.keys():\n",
    "        f.write(d)\n",
    "        f.write('\\n')\n",
    "\n",
    "# Now vocab.txt and merges.txt are added to the assets directory\n",
    "\n",
    "vocab_pth = f\"model/test/{MODEL_NAME_TF}_tokenizer/vocab.txt\"\n",
    "merges_pth = f\"model/test/{MODEL_NAME_TF}_tokenizer/merges.txt\"\n",
    "saved_model_pth = f'model/test/{MODEL_NAME_TF}_classificator_tf/saved_model/1/assets'\n",
    "\n",
    "!cp $vocab_pth $saved_model_pth\n",
    "!cp $merges_pth $saved_model_pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import it into Spark NLP\n",
    "\n",
    "embeddings = RoBertaEmbeddings.loadSavedModel('model/test/roberta_tf_classificator_tf/saved_model/1', spark).setInputCols([\"document\",'token']).setOutputCol(\"embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sparknlp.annotator.RoBertaEmbeddings"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It is loaded into Spark 游쑆n",
    "\n",
    "type(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings is the Spark NLP version of the model - let's save it\n",
    "\n",
    "embeddings.write().overwrite().save(f\"model/test/{MODEL_NAME_TF}_spark_nlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/irene/MEGAsync/EBI/repos/evidence_datasource_parsers/exploration/stopReasons/roberta-base-biomedical-es.zip'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.make_archive(\n",
    "    base_name=f\"model/test/roberta-base-biomedical-es\",\n",
    "    format=\"zip\",\n",
    "    f\"model/test/roberta_tf_spark_nlp\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Make predictions within a Spark NLP Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentAssembler = (DocumentAssembler()\n",
    "    .setInputCol(\"text\")\n",
    "    .setOutputCol(\"document\")\n",
    ")\n",
    "\n",
    "tokenizer = (Tokenizer()\n",
    "    .setInputCols(\"document\")\n",
    "    .setOutputCol(\"token\")\n",
    ")\n",
    "\n",
    "sequenceClassifier = (embeddings\n",
    "    .setInputCols([\"document\", \"token\"])\n",
    "    .setOutputCol(\"class\")\n",
    "    .setCaseSensitive(True)\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages = [\n",
    "    documentAssembler,\n",
    "    tokenizer,\n",
    "    sequenceClassifier])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(nct_id='NCT01313390', text='Lack of recruitment')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "\n",
    "CHEMBL_EVIDENCE_PATH = 'data/chembl-2021-08-23.json.gz'\n",
    "\n",
    "stopReasons = (\n",
    "        spark.read.json(CHEMBL_EVIDENCE_PATH)\n",
    "\n",
    "        # Extract a test set\n",
    "        .sample(0.01)\n",
    "\n",
    "        # Extract studies with their reasons to stop\n",
    "        .withColumn('urls', F.explode('urls'))\n",
    "        .filter(F.col('urls.niceName').contains('ClinicalTrials'))\n",
    "        .withColumn('nct_id', F.element_at(F.split(F.col('urls.url'), '%22'), -2))\n",
    "        .select('nct_id', F.col('studyStopReason').alias('text'))\n",
    "        .filter(F.col('studyStopReason').isNotNull())\n",
    "        .distinct()\n",
    "    )\n",
    "\n",
    "stopReasons.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipeline.fit(stopReasons).transform(stopReasons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nct_id: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- class: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.select('nct_id', 'text', 'class').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Labels of the classification need to be prepared\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Can I train the model using Spark NLP?\n",
    "\n",
    "I will follow this notebook as a reference: https://www.kaggle.com/pranavkasela/bert-vs-spark-nlp-use-embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "\n",
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "# from sparknlp.embeddings import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sparkContext=spark.sparkContext, \n",
    "                        sparkSession=spark)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f4b6dc36c032161cfa2dcc93f31fcb0bfb11bc6fea6f0772b15a0710e4778680"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
